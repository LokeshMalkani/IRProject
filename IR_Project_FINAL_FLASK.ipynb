{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRProjectFINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9vQPNbkarNxD",
        "Ux2anpeFsMSO",
        "SAURS0SSszWW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vQPNbkarNxD"
      },
      "source": [
        "### **Installing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH3aBTkhqmwo",
        "outputId": "3032dcf9-d562-41f1-dcea-9d8afd41faf7"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcW4G51RwWMf",
        "outputId": "0892121e-4d58-4f9d-eeb7-21b128b03d76"
      },
      "source": [
        "!pip install demoji\n",
        "!pip install transformers\n",
        "!pip install Sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.4.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.4MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting Sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.4MB/s \n",
            "\u001b[?25hInstalling collected packages: Sentencepiece\n",
            "Successfully installed Sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpHCVfzDw0yc",
        "outputId": "6aa68a6d-5b1d-499e-864a-dd158040d29e"
      },
      "source": [
        "!python -m nltk.downloader all"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux2anpeFsMSO"
      },
      "source": [
        "### **Integrating Colab with google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnXzPCquro15",
        "outputId": "4cd3f021-001d-4a03-e15a-92e3fde2e83f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAURS0SSszWW"
      },
      "source": [
        "### **Installing requirements.txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Zmu3v7mHswkj",
        "outputId": "09a18850-694f-48e3-87a7-db17ee43badb"
      },
      "source": [
        "# Installng requirements.txt file\n",
        "\n",
        "from google.colab import files\n",
        "from flask import send_from_directory\n",
        " \n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c217ee45-2504-4b97-bf7c-06cdcd936f75\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c217ee45-2504-4b97-bf7c-06cdcd936f75\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving requirements.txt to requirements.txt\n",
            "User uploaded file \"requirements.txt\" with length 1885 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W-hHJc9tOd-",
        "outputId": "3e55e72e-4002-49de-d2dd-2f7ac23ae982"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.2.0)\n",
            "Collecting cachetools==4.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/28/c4f5796c67ad06bb91d98d543a5e01805c1ff065e08871f78e52d2a331ad/cachetools-4.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: certifi==2020.12.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2020.12.5)\n",
            "Collecting chardet==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: colorama==0.4.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.4.4)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.10.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.4.2)\n",
            "Collecting delayed==0.11.0b1\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/80/96302b67fe8d324af597748d5eef9cfb98bb1e6590b5f25a5b58b5e6f93f/delayed-0.11.0b1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: demoji==0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.4.0)\n",
            "Collecting et-xmlfile==1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.0.12)\n",
            "Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (1.12)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Collecting google-auth==1.30.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/c1/44179a1cfc5c3b5832a5f9c925161612471ec5f346bcd186235651d74f35/google_auth-1.30.0-py2.py3-none-any.whl (146kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.4.4)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (0.2.0)\n",
            "Collecting grpcio==1.34.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/d1/f38a91d8724706427fe973a7dfa11e938cee98aa7196b03d870a25a08bab/grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 19.5MB/s \n",
            "\u001b[?25hCollecting h5py==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/74/9eae2bedd8201ab464308f42c601a12d79727a1c87f0c867fdefb212c6cf/h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 55.4MB/s \n",
            "\u001b[?25hCollecting hiredis==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/33/290cea35b09c80b4634773ad5572a8030a87b5d39736719f698f521d2a13/hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (2.10)\n",
            "Collecting imbalanced-learn==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/98/dc784205a7e3034e84d41ac4781660c67ad6327f2f5a80c568df31673d1c/imbalanced_learn-0.8.0-py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 60.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (0.0)\n",
            "Collecting ipykernel==5.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/95/3a670c8b2c2370bd8631c313f42e60983b3113ffec4035940592252bd6d5/ipykernel-5.5.0-py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 56.1MB/s \n",
            "\u001b[?25hCollecting ipython==7.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/43/6dbd0610550708fc418ad027fda97b5f415da9053749641654fdacfec93f/ipython-7.21.0-py3-none-any.whl (784kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 57.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (0.2.0)\n",
            "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (1.1.0)\n",
            "Requirement already satisfied: jedi==0.18.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 31)) (0.18.0)\n",
            "Requirement already satisfied: Jinja2@ file:///home/conda/feedstock_root/build_artifacts/jinja2_1612119311452/work from file:///home/conda/feedstock_root/build_artifacts/jinja2_1612119311452/work in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 32)) (2.11.3)\n",
            "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 33)) (1.0.1)\n",
            "Collecting jupyter-client==6.1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/e8/c3cf72a32a697256608d5fa96360c431adec6e1c6709ba7f13f99ff5ee04/jupyter_client-6.1.12-py3-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-core==4.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (4.7.1)\n",
            "Requirement already satisfied: Keras==2.4.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (2.4.3)\n",
            "Collecting keras-nightly==2.5.0.dev2021032900\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/e7/53bc896aa4e11a87aac10a625c676b3a3d57d1c8d9929e4809d31fa0b7d5/keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (1.1.2)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 39)) (1.3.1)\n",
            "Requirement already satisfied: Markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 40)) (3.3.4)\n",
            "Requirement already satisfied: MarkupSafe@ file:///D:/bld/markupsafe_1610127684851/work from file:///D:/bld/markupsafe_1610127684851/work in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (1.1.1)\n",
            "Collecting matplotlib==3.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/63/74c0b6184b6b169b121bb72458818ee60a7d7c436d7b1907bd5874188c55/matplotlib-3.4.1-cp37-cp37m-manylinux1_x86_64.whl (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 54.4MB/s \n",
            "\u001b[?25hCollecting nltk==3.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (1.19.5)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 45)) (3.1.0)\n",
            "Collecting openpyxl==3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/08/595298c9b7ced75e7d23be3e7596459980d63bc35112ca765ceccafbe9a4/openpyxl-3.0.7-py2.py3-none-any.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (3.3.0)\n",
            "Requirement already satisfied: packaging==20.9 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 48)) (20.9)\n",
            "Collecting pandas==1.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/51/48f3fc47c4e2144da2806dfb6629c4dd1fa3d5a143f9652b141e979a8ca9/pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9MB 46.9MB/s \n",
            "\u001b[?25hCollecting parso==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/f0/ef6bdb1eba2dbfda60c985cd8d7b47b6ed8c6a1f5d212f39ff50b64f172c/parso-0.8.1-py2.py3-none-any.whl (93kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 51)) (0.7.5)\n",
            "Collecting Pillow==8.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/34/542152297dcc6c47a9dcb0685eac6d652d878ed3cea83bf2b23cb988e857/Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 56.2MB/s \n",
            "\u001b[?25hCollecting prompt-toolkit==3.0.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e6/4b4ca4fa94462d4560ba2f4e62e62108ab07be2e16a92e594e43b12d3300/prompt_toolkit-3.0.18-py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 56.6MB/s \n",
            "\u001b[?25hCollecting protobuf==3.15.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/4e/de63de3cd9a83d3c1753a4566b11fc9d90b845f2448a132cfd36d3cb3cd1/protobuf-3.15.8-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 55)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 56)) (0.2.8)\n",
            "Collecting Pygments==2.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/80/a52c0a7c5939737c6dca75a831e89658ecb6f590fb7752ac777d221937b9/Pygments-2.8.1-py3-none-any.whl (983kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 58)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 59)) (2.8.1)\n",
            "Collecting pytz==2021.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 51.1MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==300 (from -r requirements.txt (line 61)) (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pywin32==300 (from -r requirements.txt (line 61))\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwqcVxyrHJ_"
      },
      "source": [
        "### **Flask Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5orxP8WrHaK",
        "outputId": "58939f04-fbf8-4d85-9eeb-7463143d385c"
      },
      "source": [
        "from flask import Flask,flash, request, redirect, url_for\n",
        "from flask_ngrok import run_with_ngrok\n",
        "app = Flask(__name__,\n",
        "            static_folder='/content/drive/MyDrive/IRProject-apoorv/static',\n",
        "\t\t\t\t\t\ttemplate_folder='/content/drive/MyDrive/IRProject-apoorv/templates')\n",
        "\n",
        "run_with_ngrok(app) \n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "app.secret_key = \"IRProjectIIITD\"\n",
        "\n",
        "app.config['UPLOAD_FOLDER'] = '/content/drive/MyDrive/IRProject-apoorv/UPLOAD_FOLDER'\n",
        "from flask import render_template\n",
        "from datetime import datetime\n",
        "from flask import send_from_directory\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- Apoorv CODE START ----------------------------------------------------\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "from transformers import XLNetTokenizer, XLNetModel,XLNetForSequenceClassification\n",
        "import torch\n",
        "import transformers\n",
        "from nltk.corpus import stopwords\n",
        "import joblib\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup, XLNetConfig\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from pylab import rcParams\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from torch import nn, optim\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import seaborn as sns\n",
        "# from wordcloud import WordCloud\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask import send_file\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \t\n",
        "\t\t'''\n",
        "        params:  pandas Series  (df['Text'])\n",
        "\t\treturn:  pandas Series cleaned\n",
        "\n",
        "\t\t'''\n",
        "    \n",
        "\t\ttext = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
        "\t\ttext = demoji.replace_with_desc(text, sep=\"\")\n",
        "\t\ttext = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "\t\ttext = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text)\n",
        "\t\ttext = re.sub('\\t', ' ',  text)\n",
        "\t\ttext = re.sub(r\" +\", ' ', text)\n",
        "\n",
        "\n",
        "\t\t# Removing Stopwords\n",
        "\t\ttext = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "\t\t\n",
        "\t\t# Lemmatization\n",
        "\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\twordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV} # Pos tag, used Noun, Verb, Adjective and Adverb\n",
        "\t\tpos_tagged_text = nltk.pos_tag(text.split())\n",
        "\t\ttext =  \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "\n",
        "\t\t#-------------------------[4.   Emoticon Replacement with text]------------------\n",
        "\t\ttokenizer = RegexpTokenizer(r'\\w+')\n",
        "\t\twords=WhitespaceTokenizer().tokenize(text.lower())\n",
        "\t\tdf = pd.read_excel('/content/drive/MyDrive/IRProject-apoorv/models/Emoticons_Emojis_Text.xlsx',sheet_name='Sheet2',header=0,converters={'Emoji':str,'Text':str,'Emoticons':str})\n",
        "\t\tdictOfEmoticons=df.set_index('Emoji')['Text'].to_dict()\n",
        "\t\twords=[dictOfEmoticons.get(x) if x in dictOfEmoticons.keys() else x for x in words]\n",
        "\t\ttext=' '.join(words)\n",
        "\n",
        "\n",
        "\t\n",
        "\t\treturn text\n",
        "\n",
        "\n",
        "\n",
        "# Helper class for XLNet model\n",
        "class Dataset_test(Dataset):\n",
        "    \t\n",
        "\t\n",
        "    \n",
        "    def __init__(self, tweets,tokenizer, max_len):\n",
        "        self.tweets = tweets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.tweets[item])\n",
        "      \n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=False,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = pad_sequences(encoding['input_ids'], maxlen=120, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        input_ids = input_ids.astype(dtype = 'int64')\n",
        "        input_ids = torch.tensor(input_ids) \n",
        "\n",
        "        attention_mask = pad_sequences(encoding['attention_mask'], maxlen=120, dtype=torch.Tensor ,truncating=\"post\",padding=\"post\")\n",
        "        attention_mask = attention_mask.astype(dtype = 'int64')\n",
        "        attention_mask = torch.tensor(attention_mask)       \n",
        "\n",
        "        return {\n",
        "        'review_text': review,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask.flatten(),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# function to create data loader for XLNet\n",
        "def create_data_loader_1(df, tokenizer, max_len, batch_size):\n",
        "  ds = Dataset_test(\n",
        "    tweets = df.Text.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=1\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_result(model,test_data,test_loader,device, submission):\n",
        "    \t\n",
        "            '''\n",
        "            params:\n",
        "              1. model:  XLNet model\n",
        "              2. test_data: uploadedFile\n",
        "              3. test_loader: loader for uploaded file\n",
        "              4. device\n",
        "              5. submission: to be generated by this function, will be saved in LABELLED_FOLDER\n",
        "            '''\n",
        "\n",
        "            model = model.eval()\n",
        "            losses = []\n",
        "            acc = 0\n",
        "            counter = 0\n",
        "            predictions = []\n",
        "          \n",
        "            with torch.no_grad():\n",
        "              for d in test_loader:\n",
        "                input_ids = d[\"input_ids\"].reshape(1,120).to(device)\n",
        "                attention_mask = d[\"attention_mask\"].to(device)\n",
        "                labels = torch.tensor([1]).unsqueeze(0).to(device)\n",
        "                outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = labels)\n",
        "                loss = outputs[0]\n",
        "                logits = outputs[1]\n",
        "\n",
        "                _, prediction = torch.max(outputs[1], dim=1)\n",
        "                prediction = prediction.cpu().detach().numpy()\n",
        "                predictions.append(prediction[0])\n",
        "                \n",
        "              \n",
        "                counter += 1\n",
        "\n",
        "            predictions = np.array(predictions)\n",
        "            submission[\"label\"] = predictions\n",
        "\n",
        "            submission['label'] = submission['label'].replace([0],'PF_AG')\n",
        "            submission['label'] = submission['label'].replace([1],'NEUTRAL')\n",
        "            submission['label'] = submission['label'].replace([2],'PG_AF')\n",
        "            submission['label'] = submission['label'].replace([3],'PROVOKING')\n",
        "\n",
        "\n",
        "            submission.to_csv('/content/drive/MyDrive/IRProject-apoorv/LABELLED_FOLDER/{}'.format('labelled.csv'), index = False)\n",
        "            print(\"CREATED\")\n",
        "\n",
        "\n",
        "            # call analysis functions\n",
        "            df = pd.read_csv('/content/drive/MyDrive/IRProject-apoorv/LABELLED_FOLDER/labelled.csv')\n",
        "            # df['label'] = df['label'].replace([0],'PF_AG')\n",
        "            # df['label'] = df['label'].replace([1],'NEUTRAL')\n",
        "            # df['label'] = df['label'].replace([2],'PG_AF')\n",
        "            # df['label'] = df['label'].replace([3],'PROVOKING')\n",
        "            plotHist(df)\n",
        "            plotPie(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict(uploadedFile):\n",
        "\t\t\t\t\t\t'''\n",
        "\t\t\t\t\t\tparams:  csv file which is uploaded\n",
        "\t\t\t\t\t\treturn:  predictions\n",
        "\n",
        "\t\t\t\t\t\t'''\n",
        "    \n",
        "\t\t\t\t\t\t# read csv\n",
        "\t\t\t\t\t\tdf = pd.read_csv(uploadedFile)\n",
        "\n",
        "\t\t\t\t\t\t# clean data\n",
        "\t\t\t\t\t\tdf['Text'] = df['Text'].apply(clean_text)\n",
        "\n",
        "\t\t\t\t\t\t# When deploying on heroku, change the parameter to gpu\n",
        "\t\t\t\t\t\tdevice = torch.device('cuda')#cpu')\n",
        "\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tPRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "\t\t\t\t\t\ttokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\t\tconfig = XLNetConfig.from_json_file('/content/drive/MyDrive/IRProject-apoorv/models/config.json')\n",
        "\t\t\t\t\t\tmodel = XLNetForSequenceClassification.from_pretrained('/content/drive/MyDrive/IRProject-apoorv/models/pytorch_model.bin',config=config)\n",
        "\t\t\t\t\t\tmodel = model.to(device)\n",
        "\t\t\t\t\t\n",
        "\n",
        "                       \n",
        "\t\t\t\t\t\tmodel.load_state_dict(torch.load('/content/drive/MyDrive/IRProject-apoorv/models/xlnet_model .bin', map_location=torch.device('cuda')))\n",
        "\t\t\t\t\t\tmodel = model.to(device)\n",
        "\n",
        "\n",
        "\t\t\t\t\t\tsubmission=pd.DataFrame(columns={\"Text\",\"label\"})\n",
        "\t\t\t\t\t\tsubmission[\"Text\"]= df[\"Text\"]\n",
        "\t\t\t\t\t\tdf['Text'] = df['Text'].apply(clean_text)\n",
        "\n",
        "\n",
        "\t\t\t\t\t\ttest_loader = create_data_loader_1(df,tokenizer, 120, 1)\n",
        "\t\t\t\t\t\tres = predict_result(\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tmodel,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tuploadedFile,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_loader,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tdevice,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tsubmission\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
        "\n",
        "\t\t\t\t\t\t\n",
        "\n",
        "\n",
        "\t\t\t\t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function to generate different charts for labelled data\n",
        "def plotHist(df):\n",
        "    \t\n",
        "\t\t\t\n",
        "\t\t\t# change labels\n",
        "\n",
        "\t\t\tdf['label'] = df['label'].replace(['PF_AG'],'PRO-FARMER/ANTI-GOVT')\n",
        "\t\t\tdf['label'] = df['label'].replace(['PG_AF'],'PRO-GOVT/ANTI-FARMER')\n",
        "\n",
        "\n",
        "\n",
        "\t\t\tsns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t# generate countplot\n",
        "\t\t\tplt.figure(figsize =(32, 20))\n",
        "\t\t\tb = sns.countplot(x ='label', data = df, palette=\"Set3\")\n",
        "\t\t\tb.axes.set_title(\"Count of Public Sentiments\",fontsize=50)\n",
        "\t\t\tb.set_xlabel(\"Sentiments\",fontsize=32)\n",
        "\t\t\tb.set_ylabel(\"Count\",fontsize=40)\n",
        "\t\t\tb.tick_params(labelsize=35)\n",
        "\t\t\tplt.xticks(rotation=16)\n",
        "\t\t\t\n",
        "\t\t\tplt.savefig(\"/content/drive/MyDrive/IRProject-apoorv/static/hist.png\")\n",
        "\t\t\t#plt.show()\n",
        "\t\t\n",
        "\n",
        "\n",
        "def plotPie(df):\n",
        "    \t\n",
        "\t\timport matplotlib.pyplot as plt\n",
        "    \t\n",
        "\t\tsns.set_style(\"whitegrid\")\n",
        "\n",
        "\n",
        "\t\tdf['label'] = df['label'].replace(['PF_AG'],'Pro farmer/Anti govt')\n",
        "\t\tdf['label'] = df['label'].replace(['PG_AF'],'Pro govt/Anti farmer')\n",
        "\n",
        "\t\t# generate pie chart\n",
        "\t\n",
        "\t\tL = df['label'].value_counts().tolist()\n",
        "\t\t\n",
        "\n",
        "\n",
        "\t\t\n",
        "\t\tmycolors = [\"#f5f5f5\", \"hotpink\", \"#4dd0e1\", \"#4CAF50\"]\n",
        "\t\tplt.figure(figsize =(15, 12))\n",
        "\t\tplt.pie(L, labels=df.label.value_counts().index.tolist(),shadow = True, colors=mycolors, autopct='%1.2f%%', textprops={'size':22})\n",
        "\t\tplt.title('Proportion of Public Sentiments', fontdict={'fontsize':30})\n",
        "\t\tplt.savefig(\"/content/drive/MyDrive/IRProject-apoorv/static/pie.png\")\n",
        "\n",
        "\t\t\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# ---------------- Apoorv & Anshuman CODE END ----------------------------------------------------\t                   \n",
        "\n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.context_processor\n",
        "def inject_now():\n",
        "\treturn {'now': datetime.utcnow()}\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "\treturn render_template('index.html')\n",
        "\n",
        "@app.route('/analyze_by_tweet', methods=['POST'])\n",
        "def tweetAnalyze():\n",
        "\tif request.method == 'POST':\n",
        "\t\t\tpasted_tweet = request.form.get(\"tweet\")\n",
        "\t\t\tcleaned_tweet = clean_text(pasted_tweet)\n",
        "\n",
        "\t\t\tdevice = torch.device('cuda')#cpu')\n",
        "\n",
        "\t\t\tPRE_TRAINED_MODEL_NAME = 'xlnet-base-cased'\n",
        "\t\t\ttokenizer = XLNetTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "\t\t\tconfig = XLNetConfig.from_json_file('/content/drive/MyDrive/IRProject-apoorv/models/config.json')\n",
        "\t\t\tmodel = XLNetForSequenceClassification.from_pretrained('/content/drive/MyDrive/IRProject-apoorv/models/pytorch_model.bin',config=config)\n",
        "\t\t\tmodel = model.to(device)\n",
        "\n",
        "\n",
        "\t\t\t# When deploying on heroku, change the parameter to gpu\n",
        "\t\t\tmodel.load_state_dict(torch.load('/content/drive/MyDrive/IRProject-apoorv/models/xlnet_model .bin', map_location=torch.device('cuda')))\n",
        "\t\t\tmodel = model.to(device)\n",
        "\n",
        "\t\t\tinputs = tokenizer(cleaned_tweet, return_tensors=\"pt\").to(device)\n",
        "\t\t\tlabels = torch.tensor([1]).unsqueeze(0).to(device)  # Batch size 1\n",
        "\t\t\toutputs = model(**inputs, labels=labels)\n",
        "\t\t\tloss = outputs.loss\n",
        "\t\t\tlogits = outputs.logits\n",
        "\t\t\t_, prediction = torch.max(outputs[1], dim=1)\n",
        "\t\t\tprediction = prediction.cpu().detach().numpy()\n",
        "\t\t\tsentiment = prediction[0]\n",
        "\n",
        "\t\t\tif(sentiment == 0):\n",
        "    \t\t\t\tsentiment = 'PRO-FARMER/ANTI-GOVT'\n",
        "\t\t\telif(sentiment == '1'):\n",
        "    \t\t\t\tsentiment = 'NEUTRAL'\n",
        "\t\t\telif(sentiment == '2'):\n",
        "    \t\t\t\tsentiment = 'PRO-GOVT/ANTI-FARMER'\n",
        "\t\t\telif(sentiment == '3'):\n",
        "    \t\t\t\tsentiment = 'PROVOKING'\n",
        "\t\t\t\n",
        "\t\t\tprint(sentiment)\n",
        "    \t\t\t\t\n",
        "\n",
        "\n",
        "\treturn render_template('index.html', result = sentiment, tweet = pasted_tweet)\n",
        "\n",
        "def time_in_range(start, end, x):\n",
        "    \"\"\"Return true if x is in the range (start, end)\"\"\"\n",
        "    if start < end:\n",
        "        return start < x < end\n",
        "    else:\n",
        "        return start < x or x < end\n",
        "\n",
        "def createDict(startDate,endDate):\n",
        "\t\tlistOfReslts=[]\n",
        "\t\t\n",
        "\t\tdate_1=datetime(2020,9,24)\n",
        "\t\tdate_2=datetime(2020,11,25)\n",
        "\t\tdate_3=datetime(2021,1,24)\n",
        "\t\tdate_4=datetime(2021,1,26)\n",
        "\t\tdate_5=datetime(2021,3,6)\n",
        "\t\t# date_6=datetime(2020,9,24)\n",
        "\t\t# date_7=datetime(2020,9,24)\n",
        "\n",
        "\t\t#YMD\n",
        "\t\tif endDate<date_1:\n",
        "\t\t\tpass\n",
        "\t\t\t#Do Nothing\n",
        "\t\tif time_in_range(date_1,date_2,startDate):#------------Valid------------------\n",
        "\t\t\tlistOfReslts.append(\"{}: Bill Passed\".format(date_1.strftime(\"%d/%m/%Y\")))\n",
        "\t\t\t#1st SLot\n",
        "\t\tif time_in_range(date_2,date_3,startDate):\n",
        "\t\t\tlistOfReslts.append(\"{}: Farmer Protest Started\".format(date_2.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif time_in_range(date_2,date_3,endDate): \n",
        "\t\t\tlistOfReslts.append(\"{}: Farmer Protest Started\".format(date_2.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif startDate<date_2 and endDate >date_3:\n",
        "\t\t\tlistOfReslts.append(\"{}: Farmer Protest Started\".format(date_2.strftime(\"%d/%m/%Y\")))\n",
        "\n",
        "\t\tif time_in_range(date_3,date_4,startDate):\n",
        "\t\t\tlistOfReslts.append(\"{}: Tractor Rally Started\".format(date_3.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif time_in_range(date_3,date_4,endDate): \n",
        "\t\t\tlistOfReslts.append(\"{}: Tractor Rally Started\".format(date_3.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif startDate<date_3 and endDate >date_4:\n",
        "\t\t\tlistOfReslts.append(\"{}: Tractor Rally Started\".format(date_3.strftime(\"%d/%m/%Y\")))\n",
        "\n",
        "\n",
        "\t\tif time_in_range(date_4,date_5,startDate):\n",
        "\t\t\tlistOfReslts.append(\"{}: Shameful Event on Red-Fort\".format(date_4.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif time_in_range(date_4,date_5,endDate): \n",
        "\t\t\tlistOfReslts.append(\"{}: Shameful Event on Red-Fort\".format(date_4.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif startDate<date_4 and endDate >date_5:\n",
        "\t\t\tlistOfReslts.append(\"{}: Shameful Event on Red-Fort\".format(date_4.strftime(\"%d/%m/%Y\")))\n",
        "\n",
        "\n",
        "\t\tif time_in_range(date_5,datetime.now(),startDate):\n",
        "\t\t\tlistOfReslts.append(\"{}: Covid 2nd Wave\".format(date_5.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif time_in_range(date_5,datetime.now(),endDate): \n",
        "\t\t\tlistOfReslts.append(\"{}: Covid 2nd Wave\".format(date_5.strftime(\"%d/%m/%Y\")))\n",
        "\t\telif startDate<date_5 and endDate >datetime.now():\n",
        "\t\t\tlistOfReslts.append(\"{}: Covid 2nd Wave\".format(date_5.strftime(\"%d/%m/%Y\")))\n",
        "\n",
        "\n",
        "\n",
        "\t\tif startDate>datetime.now():\n",
        "\t\t\tpass\n",
        "\t\t\t#Do Nothing\n",
        "\t\treturn listOfReslts\t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/analyze_by_csv', methods=['GET','POST'])\n",
        "def csvAnalyze():\n",
        "            if request.method == 'POST':\n",
        "                  \n",
        "                # check if the post request has the file part\n",
        "                if 'file' not in request.files:\n",
        "                  return 'No file part'\n",
        "                \n",
        "\n",
        "                file = request.files['file']\n",
        "\n",
        "                if file.filename == '':\n",
        "                  return 'No selected file'\n",
        "                  \n",
        "                if file:\n",
        "                  filename = secure_filename(file.filename)\n",
        "                  file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "                  print(\"FILE saved\")\n",
        "                  res = predict('/content/drive/MyDrive/IRProject-apoorv/UPLOAD_FOLDER/{}'.format(filename))\n",
        "\n",
        "\n",
        "                  startDate =  datetime. strptime(request.form.get('startDate'), '%m/%d/%Y')\n",
        "                  endDate = datetime. strptime(request.form.get('endDate'), '%m/%d/%Y')\n",
        "                  listOfReslts=createDict(startDate,endDate)\n",
        "\n",
        "                 \n",
        "                \n",
        "            \n",
        "            return render_template('results.html',resultlist=listOfReslts)\n",
        "\t\t\t\n",
        "\t\t\n",
        "\n",
        "@app.route('/analyze_by_date', methods=['POST'])\n",
        "def dateAnalyze():\n",
        "\tstartDate = request.form.get('startDate')\n",
        "\tendDate = request.form.get('endDate')\n",
        "\tprint(request.form, request.args, request.values)\n",
        "\treturn 'Start, end dates : {} ==> {}'.format(startDate, endDate)\n",
        "\n",
        "@app.route('/about')\n",
        "def about():\n",
        "\treturn render_template('about.html')\n",
        "\n",
        "\n",
        "@app.route('/help')\n",
        "def help():\n",
        "\treturn 'Help page, will finish later'\n",
        "\n",
        "@app.route('/downloas_csv', methods=['GET'])\n",
        "def download_csv():\n",
        "      print(\"HELLO download\")\n",
        "      path = '/content/drive/MyDrive/IRProject-apoorv/LABELLED_FOLDER/labelled.csv'\n",
        "      return send_file(path, as_attachment=True)\n",
        "\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "\tapp.run()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.16 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://f462b9e89261.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/May/2021 07:00:13] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:14] \"GET /static/main.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:14] \"GET /static/sidebar-themes.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:14] \"GET /resources/demos/style.css HTTP/1.1\" 404 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:14] \"GET /static/jquery.mCustomScrollbar.concat.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:14] \"GET /static/main.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:14] \"GET /static/first.PNG HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:15] \"GET /static/second.jpg HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:15] \"GET /static/third.jpeg HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:15] \"GET /static/fourth.jpg HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:15] \"GET /static/fifth.jpg HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:16] \"GET /static/favicon.png HTTP/1.1\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FILE saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/May/2021 07:00:39] \"POST /analyze_by_csv HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:39] \"GET /resources/demos/style.css HTTP/1.1\" 404 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:40] \"GET /static/hist.png HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:00:40] \"GET /static/pie.png HTTP/1.1\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FILE saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [10/May/2021 07:01:13] \"POST /analyze_by_csv HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [10/May/2021 07:01:13] \"GET /resources/demos/style.css HTTP/1.1\" 404 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lw27ZXJwbxc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDf-uLnBwLbg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}