{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BASELINE_CODE2_DM_DBOW_MODEL.ipynb","provenance":[],"collapsed_sections":["Ld1_rzvJod8T","QbZsme1Boo_c","MVul7HRFo5O7","qjMqpXjHo8Vv","aR9M4BVP6cT1","TbULfT5LCQqr"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ld1_rzvJod8T"},"source":["#Import Section:\n","-----------------------"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ik0hu3MM_3zJ","outputId":"0bdfe079-753b-435d-c8a4-f488057d95c9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QbZsme1Boo_c"},"source":["# Installs & Downloads:"]},{"cell_type":"code","metadata":{"id":"sqDqWAbnJ_hg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vE6v3dgaostF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6d935a4-7e39-4b63-ef1e-a4a57032410b"},"source":["!pip install demoji\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting demoji\n","  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n","Installing collected packages: colorama, demoji\n","Successfully installed colorama-0.4.4 demoji-0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-a2Wzyc_oZpl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"048c1f25-e2b5-468d-c917-f9b087339c03"},"source":["import gensim\n","import nltk\n","import numpy\n","import tensorflow\n","import torch\n","from nltk.tokenize import WhitespaceTokenizer\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import text_to_word_sequence\n","from keras.models import *\n","from keras.layers import *\n","from keras.callbacks import *\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import random\n","from nltk.stem import WordNetLemmatizer \n","import matplotlib.pyplot as plt \n","from keras.models import Model\n","from keras.layers import Dense, Input, LSTM, GRU, Conv1D, MaxPooling1D, Concatenate  ,SimpleRNN\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.initializers import glorot_uniform\n","from keras.models import Sequential \n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt\n","import random\n","import glob\n","%matplotlib inline\n","import demoji\n","import nltk\n","\n","\n","\n","from nltk.corpus import wordnet\n","\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.util import ngrams\n","import collections\n","import nltk\n","from collections import Counter\n","import re\n","from nltk.tokenize import RegexpTokenizer\n","\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.sentiment import vader\n","import multiprocessing as mp\n","import numpy as np\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import pandas as pd\n","\n","from nltk.corpus import stopwords\n","demoji.download_codes()\n","\n","nltk.download('all')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading emoji data ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["... OK (Got response in 0.39 seconds)\n","Writing emoji data to /root/.demoji/codes.json ...\n","... OK\n","[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"MVul7HRFo5O7"},"source":["#Helpers:\n"]},{"cell_type":"code","metadata":{"id":"LZwALJtVo6c_","cellView":"code"},"source":["#@title Default title text\n","# def count_hashTag(listOfPreProcessSent):\n","#   dicFinalLists =[{\"hasTagCount\":len(re.findall('\\B#\\w\\w+',text))} for text in listOfPreProcessSent]\n","#   final_df = pd.DataFrame(dicFinalLists)\n","#   return final_df\n","#   \"\"\"Count the Hashss Tags Heres\"\"\"\n","\n","\n","\n","# #---------------------------------------------COUNT Punctuations--------------------------------------------------\n","\n","# def countPnct(listOfPreProcessSent):\n","#     \"\"\"Count the Pnctutaions Tags Heres\"\"\"\n","#     dicFinalLists =[{\"pncICount\":len(re.findall(r'\\B[?|!]*([?!])\\B', text))} for text in listOfPreProcessSent]\n","#     final_df = pd.DataFrame(dicFinalLists)\n","#     return final_df\n","    \n","#     # min. 2 or more\n","\n","# # \"\"\"Mathes: :( :) :P :p :O :3 :| :/ :\\ :$ :* :@\n","# # :-( :-) :-P :-p :-O :-3 :-| :-/ :-\\ :-$ :-* :-@\n","# # :^( :^) :^P :^p :^O :^3 :^| :^/ :^\\ :^$ :^* :^@\n","# # ): (: $: *:\n","# # )-: (-: $-: *-:\n","# # )^: (^: $^: *^:\n","# # <3 </3 <\\3\n","# # :smile: :hug: :pencil:\"\"\"    \n","# def countEmoticons(text):\n","#     POSITIVE = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n","#                 \":P\", \":D\", \":d\", \":p\",\n","#                 \";P\", \";D\", \";d\", \";p\",\n","#                 \":-)\", \";-)\", \":=)\", \";=)\",\n","#                 \":<)\", \":>)\", \";>)\", \";=)\",\n","#                 \"=}\", \":)\", \"(:;)\",\n","#                 \"(;\", \":}\", \"{:\", \";}\",\n","#                 \"{;:]\",\n","#                 \"[;\", \":')\", \";')\", \":-3\",\n","#                 \"{;\", \":]\",\n","#                 \";-3\", \":-x\", \";-x\", \":-X\",\n","#                 \";-X\", \":-}\", \";-=}\", \":-]\",\n","#                 \";-]\", \":-.)\",\n","#                 \"^_^\", \"^-^\"]\n","\n","#     NEGATIVE = [\":(\", \";(\", \":'(\",\n","#                 \"=(\", \"={\", \"):\", \");\",\n","#                 \")':\", \")';\", \")=\", \"}=\",\n","#                 \";-{{\", \";-{\", \":-{{\", \":-{\",\n","#                 \":-(\", \";-(\",\n","#                 \":,)\", \":'{\",\n","#                 \"[:\", \";]\"\n","#                 ]\n","#     emoticon_string = r\"\"\"\n","#       (?:\n","#         [<>]?\n","#         [:;=8]                     # eyes\n","#         [\\-o\\*\\']?                 # optional nose\n","#         [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n","#         |\n","#         [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n","#         [\\-o\\*\\']?                 # optional nose\n","#         [:;=8]                     # eyes\n","#         [<>]?\n","#       )\"\"\"            \n","#     emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n","#     \"\"\"Count the Emoticons Tags Heres from Christopher regex\"\"\"\n","#     emoticonList=re.findall(emoticon_re,text)\n","       \n","#     posList=[e for e in emoticonList if e in POSITIVE] \n","#     negList=[e for e in emoticonList if e in NEGATIVE]\n","#     lastOne=0\n","#     # print(text.split())\n","#     lastText=text.split()[-1]\n","#     if len(emoticonList)>0 and lastText in emoticonList:\n","#         if (emoticonList[-1]) in posList:\n","#             lastOne=1#Possitive\n","#         else:\n","#             lastOne=-1#Negetive\n","#     #\"Positive\":\"Negative\":\"Last\":\n","#     return  pd.Series(np.array([len(posList),len(negList),lastOne]))\n","# def countElongated(listOfPreProcessSent):\n","#     \"\"\"Count the Elongateds Tags Heres\"\"\"\n","\n","#     dicFinalLists =[{\"elongatedCount\":len(re.findall(r'(\\w*)(\\w+)(\\2)(\\w*)', text))} for text in listOfPreProcessSent]\n","#     final_df = pd.DataFrame(dicFinalLists)\n","#     return final_df\n","        \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSGbwhkdSGcA"},"source":["# dict_sad={\":-(\":\"SAD\", \":(\":\"SAD\", \":-|\":\"SAD\",  \";-(\":\"SAD\", \";-<\":\"SAD\", \"|-{\":\"SAD\"}\n","# dict_happy={\":-)\":\"HAPPY\",\":)\":\"HAPPY\", \":o)\":\"HAPPY\",\":-}\":\"HAPPY\",\";-}\":\"HAPPY\",\":->\":\"HAPPY\",\";-)\":\"HAPPY\"}\n","\n","# #THE INPUT TEXT#\n","# a=\"guys beautifully done :-)\" \n","\n","# for i in a.split():\n","#     for j in dict_happy.keys():\n","#         if set(j).issubset(set(i)):\n","#             print \"HAPPY\"\n","#             continue\n","#     for k in dict_sad.keys():\n","#         if set(k).issubset(set(i)):\n","#             print \"SAD\"\n","#             continue\n","#     if str(i)==i.decode('utf-8','replace'):\n","#        print i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sulRBCIFpKnc"},"source":["\n","# # def findTheEMoticons(text):\n","# #   exactMatch = re.compile(ur\"([^\\.]*\\bтурција\\b[^\\.]*)\\.\", re.UNICODE)\n","# #   print exactMatch.pattern\n","# #   result= exactMatch.findall(u\"турција е на врвот од индустријата. турција е на врвот од индустријата.\")\n","    \n","#   # posList=len(re.findall( ru\"[\\U0001f600-\\U0001f650]\", text))\n","#   # return pd.Series(np.array([len(posList)]))\n","\n","\n","# import emoji\n","\n","# def extract_emojis(s):\n","#   return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n","# # \n","\n","\n","# demoji.replace_with_desc(\"game is on 🔥 🔥\", sep=\"\")#replace_with_desc(#\"game is on 🔥 🔥\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjMqpXjHo8Vv"},"source":["#Preprocess:"]},{"cell_type":"code","metadata":{"id":"Z4uSXEp7o-LV"},"source":["#======================================================================================================\n","                       \n","                                       #PreProcessing\n","\n","#==============================================================================================\n","def replaceElongated(word):\n","    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon(In Most Cases Working) \"\"\"\n","\n","    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n","    repl = r'\\1\\2\\3'\n","    if wordnet.synsets(word):\n","        return word\n","    repl_word = repeat_regexp.sub(repl, word)\n","    if repl_word != word:      \n","        return replaceElongated(repl_word)\n","    else:       \n","        return repl_word\n","\n","def doPreProcess(text):\n","\n","   # ----------------------[1.    Demoji]----------------------------------------------\n","  text=demoji.replace_with_desc(text, sep=\"\")# Repalce with text\n","\n","\n","  #------------------------[2.   # and @ handling here]--------------------------------\n","  #Eiether-------------------------------------------\n","    # text=re.sub(\"([@#][A-Za-z0-9]+)\",\"\",text)#Replace by space those words with # and @\n","\n","      #Or\n","  text=re.sub(\"([@#])\",\"\",text)#Replace by space the chars as  # and @\n","  #-----------------------------------------------------------\n","\n","\n","  #------------------------[3.    Remove the Links]--------------------------------\n","  text=re.sub(r\"http\\S+\", \"\", text)#Remove the Links\n"," \n","\n","  #-------------------------[4.   Emoticons Replacement with text]-------------------\n","  \n","\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  words=WhitespaceTokenizer().tokenize(text.lower())#tokenizer.tokenize(text)#Tokenize and casings\n","  df = pd.read_excel('/content/drive/MyDrive/Scraped Tweets/Emoticons_Emojis_Text.xlsx',sheet_name='Sheet2',header=0,converters={'Emoji':str,'Text':str,'Emoticons':str})\n","  dictOfEmoticons=df.set_index('Emoji')['Text'].to_dict()\n","  words=[dictOfEmoticons.get(x) if x in dictOfEmoticons.keys() else x for x in words]\n","  text=' '.join(words)\n","\n","\n","\n","  #----------------------[5.       Ignore Non Ascii Characters]-----------------------\n","  encoded_string = text.encode(\"ascii\", \"ignore\")  #For ASCII Onlys\n","  text = encoded_string.decode()\n","\n","  #----------------------[6.       Single Quotes Handling Here Removal]---------------------:\n","  text=text.translate(str.maketrans(\"\",\"\", \"'\"))\n","  # text=text.translate(str.maketrans(\"\",\"\", string.punctuation))\n","\n","\n","  #---------------------[7.         Tokenizes]----------------------\n","  # tokenizer = RegexpTokenizer(r'\\w+')\n","  words=WhitespaceTokenizer().tokenize(text)\n"," \n","  #---------------------[8.         Lower Case Folding]------------------------------\n","  # words=tokenizer.tokenize(text.lower())#Tokenize and casings\n","\n","  #---------------------[9.         Replace Elongated Words]-------------------------------\n","\n","  words=[replaceElongated(x) if not wordnet.synsets(x) else x for x in  words]\n","\n","\n","\n","  #---------------------[10. & 11.        Stopwords Removal & Lematize]--------------------------------------------- \n","  wnl = WordNetLemmatizer() \n","  words=[wnl.lemmatize(word) for word in words]\n","  words=[word for word in words if word not in stopwords.words('english') ]\n","\n","  finalText= ' '.join(words)\n","\n","  #-------------------[11.                 Pnctations Removal]------------------------------\n","  return re.sub(r'[^\\w\\s]', '',finalText)\n","\n","\n","def preProcessing(df):\n","  \n","  df=df[['Text']]['Text'].apply(doPreProcess)\n","  return df\n","\n","#======================================================================================================\n","                       \n","                                       #Global Data Frames\n","\n","#==============================================================================================\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aR9M4BVP6cT1"},"source":["#Run Here:\n"]},{"cell_type":"code","metadata":{"id":"TdWDkrA26bdG"},"source":["df=pd.read_csv(\"/content/drive/MyDrive/Scraped Tweets/Copy of Working_copy_22_MAR_Ver2.csv\")\n","df['Preprocessed_tweets']=df['Text'].apply(doPreProcess)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uiwy3wxzPe2T","outputId":"3c9f09d9-486f-435d-a637-a74444bb5bac"},"source":["df['Text']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       This actually broke my heart... An elderly Kis...\n","1       I am a daughter of farmers, of course Iâ€™m go...\n","2       â€œthe power of people is stronger than the pe...\n","3       Coming together is the beginning. Keeping toge...\n","4       The farmers are more articulate and aware than...\n","                              ...                        \n","1243    AAP MLA @JarnailSinghAAP arrested for protesti...\n","1244    This is a brutal and merciless attack by Modi ...\n","1245    From the barbed wires\\n From the showers of wa...\n","1246    Orders from above - use 'misguised' for #Farme...\n","1247    AIKS is at the border with the Farmers. The Al...\n","Name: Text, Length: 1248, dtype: object"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"SMDHCgMKGSYR","outputId":"828f5fb4-81ab-40d1-aaa9-4d2442663693"},"source":["df.groupby('Label').count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Datetime</th>\n","      <th>Tweet Id</th>\n","      <th>Text</th>\n","      <th>Username</th>\n","      <th>FollowersCount</th>\n","      <th>likeCount</th>\n","      <th>retweetCount</th>\n","      <th>quoteCount</th>\n","      <th>Preprocessed_tweets</th>\n","    </tr>\n","    <tr>\n","      <th>Label</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ANTI_FARMER</th>\n","      <td>25</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>ANTI_GOVT</th>\n","      <td>124</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","      <td>272</td>\n","    </tr>\n","    <tr>\n","      <th>NEUTRAL</th>\n","      <td>127</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","      <td>330</td>\n","    </tr>\n","    <tr>\n","      <th>PROVOKING</th>\n","      <td>38</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>PRO_FARMER</th>\n","      <td>137</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","      <td>402</td>\n","    </tr>\n","    <tr>\n","      <th>PRO_GOVT</th>\n","      <td>9</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","      <td>134</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Unnamed: 0  Datetime  ...  quoteCount  Preprocessed_tweets\n","Label                              ...                                 \n","ANTI_FARMER          25        62  ...          62                   62\n","ANTI_GOVT           124       272  ...         272                  272\n","NEUTRAL             127       330  ...         330                  330\n","PROVOKING            38        48  ...          48                   48\n","PRO_FARMER          137       402  ...         402                  402\n","PRO_GOVT              9       134  ...         134                  134\n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"byKF1pYCGSNh"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX-tSE0FGSCh"},"source":["train, test = train_test_split(df, test_size=0.1, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tp3MNdLORX3r"},"source":["from gensim.models import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","from tqdm import tqdm\n","from sklearn import utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFs3d9hlGR0f"},"source":["def tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text):\n","        for word in nltk.word_tokenize(sent):\n","            if len(word) < 2:\n","                continue\n","            tokens.append(word.lower())\n","    return tokens\n","train_tagged = train.apply(\n","    lambda r: TaggedDocument(words=tokenize_text(r['Preprocessed_tweets']), tags=[r.Label]), axis=1)\n","test_tagged = test.apply(\n","    lambda r: TaggedDocument(words=tokenize_text(r['Preprocessed_tweets']), tags=[r.Label]), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QA57f1l7RkNG"},"source":["import multiprocessing\n","cores = multiprocessing.cpu_count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7nNTn4JVcQb"},"source":["# **DBOW MODEL**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSqYTIZYSyKN","outputId":"5aad2743-638e-495f-bc01-c7b5ff769c6d"},"source":["model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n","model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1123/1123 [00:00<00:00, 944496.37it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oAxzoEySx_K","outputId":"070d89b8-87e8-4162-9480-1cfe8b750318"},"source":["%%time\n","for epoch in range(30):\n","    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n","    model_dbow.alpha -= 0.002\n","    model_dbow.min_alpha = model_dbow.alpha"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1123/1123 [00:00<00:00, 1024402.65it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1450632.40it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1935965.22it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 297586.77it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 649683.23it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1200663.62it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1527303.30it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 541714.02it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1154461.62it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 655469.44it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 452251.89it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 327461.30it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 305599.39it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 474292.96it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 257163.32it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1244768.34it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 362881.62it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 312120.03it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 916917.15it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1516485.32it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1440429.17it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 271653.69it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 879589.80it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 733905.17it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 800782.62it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 528227.36it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 728344.42it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 265872.85it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 509983.04it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1349628.48it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 3.29 s, sys: 455 ms, total: 3.75 s\n","Wall time: 2.62 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zdo_1rxMSxzN"},"source":["def vec_for_learning(model, tagged_docs):\n","    sents = tagged_docs.values\n","    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n","    return targets, regressors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8T-3dfJSxhi","outputId":"7ef82c8d-bf99-49a8-9290-0fae523ee671"},"source":["from sklearn.linear_model import LogisticRegression\n","import pickle\n","\n","y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n","y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n","logreg = LogisticRegression(n_jobs=1, C=1e5)\n","# pickle.dump(logreg,open('/content/drive/MyDrive/Scraped Tweets/models/dbow.pkl','wb'))\n","logreg=pickle.load(open(\"/content/drive/MyDrive/Scraped Tweets/models/dbow.pkl\", 'rb'))\n","logreg.fit(X_train, y_train)\n","y_pred = logreg.predict(X_test)\n","from sklearn.metrics import accuracy_score, f1_score\n","print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n","print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Testing accuracy 0.32\n","Testing F1 score: 0.31002172984373266\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OkL9l46RVTeL"},"source":["# **DM MODEL**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfhtAjRWSxY_","outputId":"12ab6e7d-ac2a-4876-f8e4-b0e2a209daa9"},"source":["model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n","model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1123/1123 [00:00<00:00, 1177845.31it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1q_92hCJSxPw","outputId":"e574b927-a3b0-45e9-8306-5f223de73b6d"},"source":["%%time\n","for epoch in range(30):\n","    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n","    model_dmm.alpha -= 0.002\n","    model_dmm.min_alpha = model_dmm.alpha"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1123/1123 [00:00<00:00, 558544.22it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1506301.05it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 338668.64it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 393172.24it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 323414.13it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 690949.60it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1851495.04it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 557552.48it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 282929.08it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 359887.18it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 395848.68it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 369630.65it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 278084.98it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 772291.10it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 779705.91it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 671447.38it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 630971.65it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 680468.56it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 308461.26it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 426339.92it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1235625.23it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 256784.79it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 827367.54it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 468910.24it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1339267.38it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1306937.68it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 279719.90it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 1327565.78it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 503334.41it/s]\n","100%|██████████| 1123/1123 [00:00<00:00, 995604.18it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 6.55 s, sys: 473 ms, total: 7.03 s\n","Wall time: 4.54 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvSo9czvSxET","outputId":"600ab319-7bb7-46fb-9664-93001bdb66ff"},"source":["y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n","y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n","logreg.fit(X_train, y_train)\n","# pickle.dump(logreg,open('/content/drive/MyDrive/Scraped Tweets/models/dmm.pkl','wb'))\n","logreg=pickle.load(open(\"/content/drive/MyDrive/Scraped Tweets/models/dmm.pkl\", 'rb'))\n","y_pred = logreg.predict(X_test)\n","print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n","print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Testing accuracy 0.336\n","Testing F1 score: 0.3380622768510389\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TbULfT5LCQqr"},"source":["#Misc"]},{"cell_type":"code","metadata":{"id":"TZuYqblhnfc3"},"source":["\n","\n","  \n","  #-----------------------[Stemming/Lematize]------------------------------------\n","  # stemmer=PorterStemmer()\n","  # text=str(text)\n","\n","\n","\n"," \n","  \n","  \n"," \n","  \n","\n","\n","\n","# #-------------------------[Emoticons Replacing with their words]--------------------------------------\n","# import csv\n","# mydict={}\n","# with open('/content/Emoticons_Emojis_Text.xlsx', mode='r',encoding='unicode_escape') as infile:\n","#     reader = csv.reader(infile)\n","#     with open('coors_new.csv', mode='w') as outfile:\n","#         writer = csv.writer(outfile)\n","#         mydict = {str(rows[0]):str(rows[2]) for rows in reader}\n","\n","# import csv\n","# reader = csv.reader(open('/content/Emoticons_Emojis_Text.xlsx',encoding='utf-8'))\n","\n","# result = {}\n","# for row in reader:\n","#     key = row[0]\n","#     if key in result:\n","#         # implement your duplicate row handling here\n","#         pass\n","#     result[key] = row[2:]\n","# print(result)\n","\n","\n","\n","\n","# import codecs\n","# # with codecs.open('unicode.rst', encoding='utf-8') as f:\n","\n","\n","\n","# #-------------------------[Emoticons Replacing with their words]--------------------------------------\n","# import csv\n","# mydict={}\n","# with codecs.open('/content/Emoticons_Emojis_Text.xlsx', mode='r',encoding='utf-8') as infile:\n","#     reader = csv.reader(infile)\n","#     with codecs.open('coors_new.csv', mode='w') as outfile:\n","#         writer = csv.writer(outfile)\n","#         mydict = {rows[0]:rows[2] for rows in reader}\n","\n","\n","\n","\n","\n","\n","# import pandas as pd\n","\n","# df = pd.read_excel('/content/Emoticons_Emojis_Text.xlsx',sheet_name='Sheet2',header=0,converters={'Emoji':str,'Text':str,'Emoticons':})\n","\n","\n","\n","\n","\n","# # [x+1 if x >= 45 else x+5 for x in l]\n","# #      ||||\n","# #   \\\\\\\\\\/////\n","# #    \\\\\\\\V///\n","# # for x in l:\n","# #   if x >= 45:\n","# #     add in list x+1 \n","# #   else:\n","# #     add in list x+5\n","\n","\n","\n","\n","# dictOfEmoticons=df.set_index('Emoji')['Text'].to_dict()\n","# words=[word for word in words if word in stopwords.words('english') ]\n","\n","# words=[dictOfEmoticons.get(x) if x in dictOfEmoticons.keys() else x for x in words]\n","\n","# replaceElongated(\"youuuuuuuuuuuuuuuu\")\n","\n","\n","# words=[replaceElongated(x) if not wordnet.synsets(x) else x for x in [\"hellloooooooooooo\",\"hello\",\"hi\"] ]\n","\n","\n","\n","\n","\n","#   #------------------------[#Tags And @ Removals]------------------------------------ \n","#   # text=re.sub(\"[#@]\",\" \",text)\n","#   # text=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n","  \n","#   # -----------------------[For Ascii Words]-----------------------------------------------\n","#   # encoded_string = text.encode(\"ascii\", \"ignore\")  #For ASCII Onlys\n","#   # text = encoded_string.decode()\n","\n","  \n","  \n","#   # #Split Lower StopWordsRemoval stemming\n","#   # tokenizer = RegexpTokenizer(r'\\w+')\n","#   # words=tokenizer.tokenize(text.lower())#Tokenize and casings\n","#   # # words=text.lower().split()\n","#   # words=[word for word in words ]#if word not in set(stopwords.words('english'))]\n","#   # return ' '.join(words) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0puGEd806jOc"},"source":["\n","# preProcessing(df.head(1))\n","\n","# doPreProcess(\"Why are you so desperate and rattled Mr @anilvijminister Is Haryana out of India. Trying to stop @RahulGandhi ji from entering Haryana shows your fear and guilt for plunging farmers into a deep mess. Shame.#BJPKilledDemocracy #FarmersProtest\")\n","# text=\"â€œThis would alleviate the farmers from the grave injustice done by the #ModiGovt &amp; #BJP,â€ #KCVenugopal, MP &amp; AICC General Secretary said in a statement on Monday. FarmActs #FarmersProtest #SoniaGandhi https://t.co/aPx6UyANBZ\"\n","# words=WhitespaceTokenizer().tokenize(text)\n","# wnl = WordNetLemmatizer() \n","# words=[wnl.lemmatize(word) for word in words]\n","# words=[word for word in words if word not in stopwords.words('english') ]\n","# print(words)\n","\n"],"execution_count":null,"outputs":[]}]}