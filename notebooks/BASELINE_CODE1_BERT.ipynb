{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c3be62b64273495db93b2a72af1c25c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_577840552bf94345862d3cb313970b39",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_84a5ab2caab94ecebf184e910e744707",
              "IPY_MODEL_7582c123d1854280a00763b1d43e4c88"
            ]
          }
        },
        "577840552bf94345862d3cb313970b39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84a5ab2caab94ecebf184e910e744707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82e461410c104bff81ee99cadfd21e6d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e619ff35e59429fa75cf1c9e74708e9"
          }
        },
        "7582c123d1854280a00763b1d43e4c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4c2efd59f134578b33dfa08158ec9fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 2.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f749a1f457d040cd99258d6ed154922e"
          }
        },
        "82e461410c104bff81ee99cadfd21e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e619ff35e59429fa75cf1c9e74708e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4c2efd59f134578b33dfa08158ec9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f749a1f457d040cd99258d6ed154922e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b376766da874a6dad8ecfe0e2e61581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_212f6caa6262407abd01dc328e5de5e7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e91a0cdeafe049d2a9e8ac6203f3419e",
              "IPY_MODEL_2d42883b72c341bca95f0c919aded028"
            ]
          }
        },
        "212f6caa6262407abd01dc328e5de5e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e91a0cdeafe049d2a9e8ac6203f3419e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3c0b81605c448cc9f27ec9e5fe06e1d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_026145c5ac1f43ba95c495cf8253f0fc"
          }
        },
        "2d42883b72c341bca95f0c919aded028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1309ef6a1ddd4498a1317e25552a544a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:43&lt;00:00, 10.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da6fc4335b644247ac594243f26bddbf"
          }
        },
        "d3c0b81605c448cc9f27ec9e5fe06e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "026145c5ac1f43ba95c495cf8253f0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1309ef6a1ddd4498a1317e25552a544a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da6fc4335b644247ac594243f26bddbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e420bbb8ec974c85893cd7b66c8d0002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc0e310fd965463aa0f877f5f1d67a0e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ade14e37a82b4e89bf3e107b43067dbd",
              "IPY_MODEL_ca37589a0d6b47ee9b34750495a842ae"
            ]
          }
        },
        "fc0e310fd965463aa0f877f5f1d67a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ade14e37a82b4e89bf3e107b43067dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_54f01a248f1144f9abbf1d33f566a995",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_79045480a9f94e508ca6e4c8992561ee"
          }
        },
        "ca37589a0d6b47ee9b34750495a842ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7c130192dc61486ea2e464fca0e7cbef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:08&lt;00:00, 52.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1616cd92e48f4005954fb5f95c2402cc"
          }
        },
        "54f01a248f1144f9abbf1d33f566a995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "79045480a9f94e508ca6e4c8992561ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c130192dc61486ea2e464fca0e7cbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1616cd92e48f4005954fb5f95c2402cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGtVEUrrK-Me"
      },
      "source": [
        "https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/08.sentiment-analysis-with-bert.ipynb\n",
        "https://www.youtube.com/watch?v=8N-nM3QW7O0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywa_F72NmfpA"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import sklearn\n",
        "import tensorflow\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukTMZYQvS-Up"
      },
      "source": [
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fphyZhuHUNFA"
      },
      "source": [
        "#!pip install -qq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tksN2rsiF2sD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df49c2aa-7523-4716-cdcf-a149bc80879a"
      },
      "source": [
        "!pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\r\u001b[K     |▍                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 29.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 28.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 26.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 28.6MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 20.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 22.3MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 174kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 184kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 194kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 204kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 215kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 245kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 256kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 266kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 276kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 286kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 296kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 317kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 337kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 348kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 358kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 368kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 389kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 399kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 409kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 419kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 430kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 440kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 460kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 471kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 481kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 491kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 501kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 512kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 522kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 532kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 542kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 552kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 563kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 573kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 583kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 593kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 604kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 614kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 624kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 634kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 645kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 655kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 665kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 675kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 686kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 696kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 706kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 716kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 727kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 737kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 747kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 757kB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/82/0e82a95bd9db2b32569500cc1bb47aa7c4e0f57aa5e35cceba414096917b/tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=071fc7db5f3fe08887360ba82d0019652cc1f283d68cf2c50e16edd1127f4ab3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lcrZEN1UP1m",
        "outputId": "ef1d3821-a4e3-4ccc-fb1f-9bde7a3274ba"
      },
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.7.10\n",
            "IPython version      : 5.5.0\n",
            "\n",
            "numpy       : 1.19.5\n",
            "pandas      : 1.1.5\n",
            "torch       : 1.8.0+cu101\n",
            "transformers: 3.0.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua2aksGGUSF2",
        "outputId": "4f8d8285-124e-417b-822f-4d72ea8e81ce"
      },
      "source": [
        "#@title Setup & Config { display-mode: \"code\" }\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx1GpKNNUYmx"
      },
      "source": [
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c3be62b64273495db93b2a72af1c25c6",
            "577840552bf94345862d3cb313970b39",
            "84a5ab2caab94ecebf184e910e744707",
            "7582c123d1854280a00763b1d43e4c88",
            "82e461410c104bff81ee99cadfd21e6d",
            "7e619ff35e59429fa75cf1c9e74708e9",
            "a4c2efd59f134578b33dfa08158ec9fa",
            "f749a1f457d040cd99258d6ed154922e"
          ]
        },
        "id": "8tz85kTUUwr_",
        "outputId": "a3ebe0db-05f4-4493-c018-3977c0a99b67"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3be62b64273495db93b2a72af1c25c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV564F35U0Cz"
      },
      "source": [
        "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YvEjiFWU27T",
        "outputId": "bca2b4df-52a5-4a69-c5da-675537bc8025"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Sentence: {sample_txt}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Sentence: When was I last outside? I am stuck at home for 2 weeks.\n",
            "   Tokens: ['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n",
            "Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Zm6UU8U40y",
        "outputId": "8d585dd1-9738-47f0-824d-db6c47188bd0"
      },
      "source": [
        "# Special Tokens\n",
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[SEP]', 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-2rn1olVAui",
        "outputId": "02b54ff8-1c9d-413e-c9bd-528f4a30ffe5"
      },
      "source": [
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS]', 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHq8lHecVJpL",
        "outputId": "1ebc00b6-e9c1-4047-c1aa-919eb340fdc5"
      },
      "source": [
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[PAD]', 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JouU8osVMVm",
        "outputId": "aa4b91d9-bab9-4d87-ca1f-29dbe21ba225"
      },
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egC9G0WNVOxj",
        "outputId": "52722920-4e4e-404e-dc46-c0325a0d2631"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEznpqWnVSMC",
        "outputId": "f0c9370e-8e92-4b08-ef7a-5cfd19962907"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n",
              "        1111,  123, 2277,  119,  102,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXgL86t7VaLV",
        "outputId": "e225dc6e-47be-4b68-f347-328a3573b685"
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnjfrTTlVfOw",
        "outputId": "97f6ba56-7163-467c-e60a-a95de890d3d7"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'When',\n",
              " 'was',\n",
              " 'I',\n",
              " 'last',\n",
              " 'outside',\n",
              " '?',\n",
              " 'I',\n",
              " 'am',\n",
              " 'stuck',\n",
              " 'at',\n",
              " 'home',\n",
              " 'for',\n",
              " '2',\n",
              " 'weeks',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5SieUVSVxDE"
      },
      "source": [
        "# df = pd.read_csv(\"/content/drive/MyDrive/IR_Project/Labelled_Dataset_Arpit/2020-09-13_2020-09-19.csv\")\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IR_Project/Final_22_MAR-DATASET/Working_copy_22_MAR_Ver2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "jLgIk543WMC1",
        "outputId": "a3ed4813-7b94-4d38-8dd2-2a53a7ee25a9"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Tweet Id</th>\n",
              "      <th>Text</th>\n",
              "      <th>Username</th>\n",
              "      <th>FollowersCount</th>\n",
              "      <th>likeCount</th>\n",
              "      <th>retweetCount</th>\n",
              "      <th>quoteCount</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2020-12-01 20:27:34+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>This actually broke my heart... An elderly Kis...</td>\n",
              "      <td>HayreSay</td>\n",
              "      <td>1408</td>\n",
              "      <td>120</td>\n",
              "      <td>24</td>\n",
              "      <td>12</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>2020-12-01 19:46:56+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>I am a daughter of farmers, of course Iâ€™m go...</td>\n",
              "      <td>humneet06</td>\n",
              "      <td>646</td>\n",
              "      <td>120</td>\n",
              "      <td>39</td>\n",
              "      <td>8</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>2020-12-01 19:04:34+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>â€œthe power of people is stronger than the pe...</td>\n",
              "      <td>mansikaur_</td>\n",
              "      <td>1700</td>\n",
              "      <td>219</td>\n",
              "      <td>63</td>\n",
              "      <td>20</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.0</td>\n",
              "      <td>2020-12-01 19:04:24+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>Coming together is the beginning. Keeping toge...</td>\n",
              "      <td>khalramission</td>\n",
              "      <td>4103</td>\n",
              "      <td>169</td>\n",
              "      <td>31</td>\n",
              "      <td>18</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.0</td>\n",
              "      <td>2020-12-01 18:50:42+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>The farmers are more articulate and aware than...</td>\n",
              "      <td>parthpunter</td>\n",
              "      <td>15535</td>\n",
              "      <td>254</td>\n",
              "      <td>41</td>\n",
              "      <td>4</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:58:32+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>AAP MLA @JarnailSinghAAP arrested for protesti...</td>\n",
              "      <td>VickyKedia</td>\n",
              "      <td>6532</td>\n",
              "      <td>209</td>\n",
              "      <td>83</td>\n",
              "      <td>0</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:56:11+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>This is a brutal and merciless attack by Modi ...</td>\n",
              "      <td>sunmor2901</td>\n",
              "      <td>3977</td>\n",
              "      <td>211</td>\n",
              "      <td>142</td>\n",
              "      <td>8</td>\n",
              "      <td>ANTI_GOVT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:55:00+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>From the barbed wires\\n From the showers of wa...</td>\n",
              "      <td>SurrbhiM</td>\n",
              "      <td>5717</td>\n",
              "      <td>234</td>\n",
              "      <td>137</td>\n",
              "      <td>5</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:53:44+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>Orders from above - use 'misguised' for #Farme...</td>\n",
              "      <td>TheDeshBhakt</td>\n",
              "      <td>501907</td>\n",
              "      <td>503</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:53:35+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>AIKS is at the border with the Farmers. The Al...</td>\n",
              "      <td>Zizezkianism</td>\n",
              "      <td>6668</td>\n",
              "      <td>81</td>\n",
              "      <td>26</td>\n",
              "      <td>3</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1248 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                   Datetime  ...  quoteCount       Label\n",
              "0            2.0  2020-12-01 20:27:34+00:00  ...          12  PRO_FARMER\n",
              "1            5.0  2020-12-01 19:46:56+00:00  ...           8  PRO_FARMER\n",
              "2            8.0  2020-12-01 19:04:34+00:00  ...          20  PRO_FARMER\n",
              "3            9.0  2020-12-01 19:04:24+00:00  ...          18  PRO_FARMER\n",
              "4           10.0  2020-12-01 18:50:42+00:00  ...           4  PRO_FARMER\n",
              "...          ...                        ...  ...         ...         ...\n",
              "1243         NaN  2020-11-27 08:58:32+00:00  ...           0     NEUTRAL\n",
              "1244         NaN  2020-11-27 08:56:11+00:00  ...           8   ANTI_GOVT\n",
              "1245         NaN  2020-11-27 08:55:00+00:00  ...           5  PRO_FARMER\n",
              "1246         NaN  2020-11-27 08:53:44+00:00  ...           2  PRO_FARMER\n",
              "1247         NaN  2020-11-27 08:53:35+00:00  ...           3     NEUTRAL\n",
              "\n",
              "[1248 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "4CqT2SpTWNIP",
        "outputId": "784b234e-144b-4efa-efa9-8ec33691b7ca"
      },
      "source": [
        "class_names = ['PRO_GOVT', 'ANTI_FARMER', 'NEUTRAL', 'PROVOKING', 'PRO_FARMER', 'ANTI_GOVT']\n",
        "\n",
        "dict_class = {'PG':0, 'AF':1, 'N':2, 'P':3, 'PF':4, 'AG':5}\n",
        "\n",
        "df['numeric_Label'] = df.Label\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    df.numeric_Label[df['numeric_Label'] == class_names[i]] = i\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Tweet Id</th>\n",
              "      <th>Text</th>\n",
              "      <th>Username</th>\n",
              "      <th>FollowersCount</th>\n",
              "      <th>likeCount</th>\n",
              "      <th>retweetCount</th>\n",
              "      <th>quoteCount</th>\n",
              "      <th>Label</th>\n",
              "      <th>numeric_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2020-12-01 20:27:34+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>This actually broke my heart... An elderly Kis...</td>\n",
              "      <td>HayreSay</td>\n",
              "      <td>1408</td>\n",
              "      <td>120</td>\n",
              "      <td>24</td>\n",
              "      <td>12</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>2020-12-01 19:46:56+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>I am a daughter of farmers, of course Iâ€™m go...</td>\n",
              "      <td>humneet06</td>\n",
              "      <td>646</td>\n",
              "      <td>120</td>\n",
              "      <td>39</td>\n",
              "      <td>8</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>2020-12-01 19:04:34+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>â€œthe power of people is stronger than the pe...</td>\n",
              "      <td>mansikaur_</td>\n",
              "      <td>1700</td>\n",
              "      <td>219</td>\n",
              "      <td>63</td>\n",
              "      <td>20</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.0</td>\n",
              "      <td>2020-12-01 19:04:24+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>Coming together is the beginning. Keeping toge...</td>\n",
              "      <td>khalramission</td>\n",
              "      <td>4103</td>\n",
              "      <td>169</td>\n",
              "      <td>31</td>\n",
              "      <td>18</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.0</td>\n",
              "      <td>2020-12-01 18:50:42+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>The farmers are more articulate and aware than...</td>\n",
              "      <td>parthpunter</td>\n",
              "      <td>15535</td>\n",
              "      <td>254</td>\n",
              "      <td>41</td>\n",
              "      <td>4</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:58:32+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>AAP MLA @JarnailSinghAAP arrested for protesti...</td>\n",
              "      <td>VickyKedia</td>\n",
              "      <td>6532</td>\n",
              "      <td>209</td>\n",
              "      <td>83</td>\n",
              "      <td>0</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:56:11+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>This is a brutal and merciless attack by Modi ...</td>\n",
              "      <td>sunmor2901</td>\n",
              "      <td>3977</td>\n",
              "      <td>211</td>\n",
              "      <td>142</td>\n",
              "      <td>8</td>\n",
              "      <td>ANTI_GOVT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:55:00+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>From the barbed wires\\n From the showers of wa...</td>\n",
              "      <td>SurrbhiM</td>\n",
              "      <td>5717</td>\n",
              "      <td>234</td>\n",
              "      <td>137</td>\n",
              "      <td>5</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:53:44+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>Orders from above - use 'misguised' for #Farme...</td>\n",
              "      <td>TheDeshBhakt</td>\n",
              "      <td>501907</td>\n",
              "      <td>503</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>PRO_FARMER</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-11-27 08:53:35+00:00</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>AIKS is at the border with the Farmers. The Al...</td>\n",
              "      <td>Zizezkianism</td>\n",
              "      <td>6668</td>\n",
              "      <td>81</td>\n",
              "      <td>26</td>\n",
              "      <td>3</td>\n",
              "      <td>NEUTRAL</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1248 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                   Datetime  ...       Label numeric_Label\n",
              "0            2.0  2020-12-01 20:27:34+00:00  ...  PRO_FARMER             4\n",
              "1            5.0  2020-12-01 19:46:56+00:00  ...  PRO_FARMER             4\n",
              "2            8.0  2020-12-01 19:04:34+00:00  ...  PRO_FARMER             4\n",
              "3            9.0  2020-12-01 19:04:24+00:00  ...  PRO_FARMER             4\n",
              "4           10.0  2020-12-01 18:50:42+00:00  ...  PRO_FARMER             4\n",
              "...          ...                        ...  ...         ...           ...\n",
              "1243         NaN  2020-11-27 08:58:32+00:00  ...     NEUTRAL             2\n",
              "1244         NaN  2020-11-27 08:56:11+00:00  ...   ANTI_GOVT             5\n",
              "1245         NaN  2020-11-27 08:55:00+00:00  ...  PRO_FARMER             4\n",
              "1246         NaN  2020-11-27 08:53:44+00:00  ...  PRO_FARMER             4\n",
              "1247         NaN  2020-11-27 08:53:35+00:00  ...     NEUTRAL             2\n",
              "\n",
              "[1248 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWV0e6zyWRa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec417237-fdd9-49eb-d74f-af5c38330275"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df.Text:\n",
        "  tokens = tokenizer.encode(txt, max_length=280)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "Bs2dQMXVWg3D",
        "outputId": "43b3cbb4-9de1-4ae8-f75b-91b76d627996"
      },
      "source": [
        "\n",
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 757,
              "height": 489
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF1Ou0KxXYhZ"
      },
      "source": [
        "\n",
        "MAX_LEN = 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbITM5WvWl2g"
      },
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "U-dC76Mv6APH",
        "outputId": "72a89a26-3c3b-4647-8817-bac36b30a2ae"
      },
      "source": [
        "short_df = df[[\"Text\", \"numeric_Label\"]]\n",
        "short_df.columns = ['DATA', 'LABEL']\n",
        "short_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATA</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This actually broke my heart... An elderly Kis...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I am a daughter of farmers, of course Iâ€™m go...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>â€œthe power of people is stronger than the pe...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Coming together is the beginning. Keeping toge...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The farmers are more articulate and aware than...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                DATA LABEL\n",
              "0  This actually broke my heart... An elderly Kis...     4\n",
              "1  I am a daughter of farmers, of course Iâ€™m go...     4\n",
              "2  â€œthe power of people is stronger than the pe...     4\n",
              "3  Coming together is the beginning. Keeping toge...     4\n",
              "4  The farmers are more articulate and aware than...     4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8rxo5wdWu3p"
      },
      "source": [
        "df_train, df_test = train_test_split(short_df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9wuyzUJW01-",
        "outputId": "649d2972-b09f-4e83-9e02-803fe86a8401"
      },
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1123, 2), (62, 2), (63, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvNOCjO7X72C",
        "outputId": "6a0c65cb-6448-41fe-b494-9c72bfb8da7c"
      },
      "source": [
        "df.info(), short_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1248 entries, 0 to 1247\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Unnamed: 0      460 non-null    float64\n",
            " 1   Datetime        1248 non-null   object \n",
            " 2   Tweet Id        1248 non-null   float64\n",
            " 3   Text            1248 non-null   object \n",
            " 4   Username        1248 non-null   object \n",
            " 5   FollowersCount  1248 non-null   int64  \n",
            " 6   likeCount       1248 non-null   int64  \n",
            " 7   retweetCount    1248 non-null   int64  \n",
            " 8   quoteCount      1248 non-null   int64  \n",
            " 9   Label           1248 non-null   object \n",
            " 10  numeric_Label   1248 non-null   object \n",
            "dtypes: float64(2), int64(4), object(5)\n",
            "memory usage: 107.4+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1248 entries, 0 to 1247\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   DATA    1248 non-null   object\n",
            " 1   LABEL   1248 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 19.6+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RJt9FChW4b8"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=short_df.DATA.to_numpy(),\n",
        "    targets=short_df.LABEL.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMRvlcAzXDza",
        "outputId": "95fcc174-644e-4465-93d8-408e9e2462e1"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDxGs6K6XGB7",
        "outputId": "ecf891f9-075b-4a86-d97b-ec392e683ae8"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpFMhtHsZDQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938d3b89-1b76-4527-cc10-3415a03b1c3d"
      },
      "source": [
        "\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160])\n",
            "torch.Size([16, 160])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "4b376766da874a6dad8ecfe0e2e61581",
            "212f6caa6262407abd01dc328e5de5e7",
            "e91a0cdeafe049d2a9e8ac6203f3419e",
            "2d42883b72c341bca95f0c919aded028",
            "d3c0b81605c448cc9f27ec9e5fe06e1d",
            "026145c5ac1f43ba95c495cf8253f0fc",
            "1309ef6a1ddd4498a1317e25552a544a",
            "da6fc4335b644247ac594243f26bddbf",
            "e420bbb8ec974c85893cd7b66c8d0002",
            "fc0e310fd965463aa0f877f5f1d67a0e",
            "ade14e37a82b4e89bf3e107b43067dbd",
            "ca37589a0d6b47ee9b34750495a842ae",
            "54f01a248f1144f9abbf1d33f566a995",
            "79045480a9f94e508ca6e4c8992561ee",
            "7c130192dc61486ea2e464fca0e7cbef",
            "1616cd92e48f4005954fb5f95c2402cc"
          ]
        },
        "id": "Qpo8zHXt645h",
        "outputId": "8c12347c-3f3d-42d2-eefd-915a7d4c4b97"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b376766da874a6dad8ecfe0e2e61581",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e420bbb8ec974c85893cd7b66c8d0002",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZypW1Ws68K_"
      },
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3wjO5Bt7I0j",
        "outputId": "d34a6106-33ee-4464-d525-2ccc4e003242"
      },
      "source": [
        "\n",
        "last_hidden_state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.4070,  0.1858, -0.2489,  ..., -0.0298,  0.1574, -0.0787],\n",
              "         [ 0.2521, -0.4902,  0.4746,  ..., -0.3883,  0.2401, -0.1347],\n",
              "         [ 0.0866, -0.0274, -0.5215,  ...,  0.6011, -0.2430,  0.6801],\n",
              "         ...,\n",
              "         [ 0.0442, -0.0196,  0.2204,  ...,  0.1434,  0.1860, -0.0502],\n",
              "         [ 0.2445, -0.1682,  0.0136,  ...,  0.1560,  0.5221,  0.0610],\n",
              "         [ 0.1657, -0.0233,  0.1242,  ...,  0.1789,  0.2596,  0.0111]]],\n",
              "       grad_fn=<NativeLayerNormBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJiowr3y7LcK",
        "outputId": "1d6f2c3e-aa78-479d-cf7c-979748369f10"
      },
      "source": [
        "bert_model.config.hidden_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd2Z4CKu7cmT",
        "outputId": "0346fea6-0cdc-453c-ab65-280710e94873"
      },
      "source": [
        "\n",
        "pooled_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.1750e-01,  4.5661e-01,  9.9983e-01, -9.9218e-01,  9.6206e-01,\n",
              "          9.2055e-01,  9.8129e-01, -9.8628e-01, -9.5908e-01, -6.2142e-01,\n",
              "          9.7354e-01,  9.9778e-01, -9.9626e-01, -9.9973e-01,  6.3165e-01,\n",
              "         -9.7639e-01,  9.8318e-01, -5.3571e-01, -9.9994e-01, -6.7816e-01,\n",
              "         -1.9957e-01, -9.9982e-01,  2.8625e-01,  9.6940e-01,  9.6161e-01,\n",
              "          7.7488e-02,  9.7733e-01,  9.9994e-01,  8.0861e-01, -9.9909e-02,\n",
              "          3.0196e-01, -9.8676e-01,  8.5750e-01, -9.9825e-01,  1.6589e-01,\n",
              "          2.6509e-01,  6.9193e-01, -2.4500e-01,  9.2214e-01, -9.3336e-01,\n",
              "         -5.3641e-01, -7.4646e-01,  6.8753e-01, -5.3900e-01,  8.5151e-01,\n",
              "          9.6114e-02,  2.1984e-01, -1.5041e-01, -1.8023e-01,  9.9985e-01,\n",
              "         -9.5577e-01,  9.9827e-01, -9.9100e-01,  9.9434e-01,  9.9250e-01,\n",
              "          2.6691e-01,  9.9225e-01,  1.7253e-01, -9.9740e-01,  2.5879e-01,\n",
              "          9.3056e-01,  2.1229e-01,  8.6981e-01, -1.5434e-01,  1.0194e-02,\n",
              "         -1.2379e-01, -9.1026e-01,  2.2624e-01, -4.7767e-01,  2.6781e-01,\n",
              "          2.2926e-01,  4.2296e-01,  9.8499e-01, -8.7922e-01,  4.5683e-03,\n",
              "         -8.6756e-01,  3.8329e-01, -9.9982e-01,  9.2362e-01,  9.9994e-01,\n",
              "          6.4805e-01, -9.9954e-01,  9.9118e-01, -2.7375e-01, -5.7602e-01,\n",
              "          3.6975e-01, -9.9815e-01, -9.9924e-01,  3.8235e-02, -4.4624e-01,\n",
              "          8.9337e-01, -9.8103e-01,  4.5640e-01, -9.0727e-01,  9.9996e-01,\n",
              "         -6.3708e-01, -1.3600e-01,  3.3327e-01,  9.3882e-01, -7.3215e-01,\n",
              "         -7.1283e-01,  9.0229e-01,  9.9840e-01, -9.9307e-01,  9.9765e-01,\n",
              "          6.9577e-01, -8.9343e-01, -8.5811e-01,  6.8662e-01,  6.9560e-02,\n",
              "          9.8324e-01, -9.8582e-01, -8.5564e-01,  9.2690e-02,  9.6721e-01,\n",
              "         -8.4823e-01,  9.8281e-01,  7.5214e-01, -3.4448e-01,  9.9996e-01,\n",
              "         -2.0711e-01,  9.5273e-01,  9.9769e-01,  7.8995e-01, -7.5673e-01,\n",
              "         -2.4260e-01, -5.7191e-01,  8.0892e-01, -4.8912e-01, -2.1891e-01,\n",
              "          7.2486e-01, -9.8433e-01, -9.9671e-01,  9.9913e-01, -3.3969e-01,\n",
              "          9.9995e-01, -9.9869e-01,  9.8769e-01, -9.9990e-01, -8.0846e-01,\n",
              "         -7.2767e-01, -6.8242e-02, -9.8329e-01,  2.6888e-01,  9.8823e-01,\n",
              "          8.0636e-02, -8.5166e-01, -7.3787e-01,  6.2064e-01, -8.5063e-01,\n",
              "          4.5922e-01,  7.5684e-01, -9.3681e-01,  9.9516e-01,  9.9581e-01,\n",
              "          9.4896e-01,  9.8663e-01,  2.0609e-01, -9.3436e-01,  8.9826e-01,\n",
              "          9.7618e-01, -9.9911e-01,  7.7655e-01, -9.8894e-01,  9.9850e-01,\n",
              "          9.6381e-01,  8.1905e-01, -9.9270e-01,  9.9986e-01, -5.2630e-01,\n",
              "         -3.3286e-02, -2.3299e-01, -1.8368e-01, -9.9831e-01,  4.1431e-01,\n",
              "          4.7337e-01,  7.4069e-01,  9.9935e-01, -9.9332e-01,  9.9968e-01,\n",
              "          9.6583e-01,  1.6888e-01,  6.9869e-01,  9.9771e-01, -9.9419e-01,\n",
              "         -9.7617e-01, -9.8242e-01,  3.0618e-01,  6.1643e-01,  8.3096e-01,\n",
              "          3.3611e-01,  9.5940e-01,  9.9769e-01,  4.9463e-01, -9.9773e-01,\n",
              "         -3.6687e-01,  9.6680e-01, -2.2903e-01,  9.9995e-01,  1.7415e-01,\n",
              "         -9.9965e-01, -7.7110e-01,  9.1297e-01,  9.8540e-01, -1.7626e-01,\n",
              "          9.7302e-01, -5.8025e-01, -1.5700e-01,  9.8110e-01, -9.9630e-01,\n",
              "          9.9671e-01, -2.6423e-01,  7.7188e-01,  8.4917e-01,  9.8826e-01,\n",
              "         -7.3066e-01, -1.7182e-01,  3.1069e-01, -6.7169e-01,  9.9983e-01,\n",
              "         -9.9929e-01, -2.2332e-01,  4.3170e-01, -9.9082e-01, -9.9725e-01,\n",
              "          9.5512e-01,  4.2254e-02, -8.7628e-01, -2.0464e-01,  2.5248e-01,\n",
              "          2.7289e-01,  8.7376e-01,  9.8408e-01, -4.7612e-01, -5.3979e-01,\n",
              "         -9.9970e-01, -9.9472e-01, -7.4189e-01, -9.4490e-01,  3.7821e-02,\n",
              "          6.3805e-01, -3.4589e-01, -9.1045e-01, -9.9788e-01,  9.5929e-01,\n",
              "          5.3074e-01, -8.7993e-01, -6.7782e-02, -4.5675e-01, -9.9723e-01,\n",
              "          5.3620e-01, -8.9014e-01, -9.9828e-01,  9.9927e-01, -6.3579e-01,\n",
              "          9.9149e-01,  9.6539e-01, -9.9287e-01,  7.2322e-01, -9.9835e-01,\n",
              "         -2.0894e-01, -9.9623e-01,  2.6314e-01,  5.1770e-01, -6.3560e-01,\n",
              "         -8.2042e-02,  9.8942e-01, -9.3624e-01, -6.4109e-01,  6.6808e-01,\n",
              "         -9.9984e-01,  9.4011e-01, -3.0569e-01,  9.9844e-01,  7.3514e-01,\n",
              "          2.9682e-01,  9.7830e-01,  9.1884e-01, -9.8416e-01, -9.9970e-01,\n",
              "          9.2857e-01,  9.7788e-01, -9.9006e-01, -2.6587e-01,  9.9988e-01,\n",
              "         -9.9717e-01, -7.4670e-01, -9.3076e-01, -9.9101e-01, -9.9953e-01,\n",
              "          2.6320e-01, -7.6064e-01,  2.1926e-01,  9.7717e-01,  2.8441e-01,\n",
              "          1.5916e-01,  9.9151e-01,  9.9132e-01,  2.3328e-01,  4.0048e-02,\n",
              "          5.4571e-02, -9.5980e-01, -9.8860e-01,  5.6498e-01,  1.9501e-01,\n",
              "         -9.9994e-01,  9.9971e-01, -9.9061e-01,  9.9709e-01,  8.6070e-01,\n",
              "         -9.8983e-01,  7.7408e-01,  8.8986e-02, -9.4417e-01, -2.4748e-02,\n",
              "          9.9985e-01,  9.7333e-01, -9.9477e-02,  1.0509e-01,  8.7930e-01,\n",
              "         -1.7664e-01,  5.5802e-01, -7.0132e-01, -5.5241e-01,  1.6942e-01,\n",
              "         -9.1745e-01,  9.8912e-01,  7.7570e-01, -9.8490e-01,  9.9291e-01,\n",
              "          1.3830e-01,  5.4984e-01, -8.6783e-01,  7.9251e-01,  9.8031e-01,\n",
              "         -1.4099e-01, -4.0478e-01,  8.2899e-02, -8.5246e-01, -9.7212e-01,\n",
              "          1.1046e-01, -9.9602e-01, -2.8127e-01,  9.2528e-01,  9.7985e-01,\n",
              "         -9.8192e-01,  9.9292e-01, -8.4027e-02,  9.1851e-01, -9.9716e-01,\n",
              "          9.9997e-01, -9.9660e-01,  1.1324e-01,  6.8067e-01, -8.4778e-01,\n",
              "         -5.1708e-01,  9.8862e-01,  9.7704e-01,  9.5871e-01, -7.2747e-01,\n",
              "         -6.1217e-01,  8.2724e-01,  9.6283e-01, -9.7462e-01, -2.6161e-02,\n",
              "         -9.9901e-01, -6.4648e-01,  9.9555e-01,  9.9219e-01, -1.4376e-02,\n",
              "         -4.7375e-01, -9.9540e-01,  9.4438e-01, -7.4635e-01, -9.0579e-01,\n",
              "         -1.1626e-01, -7.6878e-01,  6.2193e-01,  9.9633e-01, -2.0217e-01,\n",
              "          6.8284e-01,  1.4244e-01, -9.8360e-01,  8.1184e-01,  6.8867e-01,\n",
              "          9.9976e-01, -9.5915e-01,  4.4877e-01,  9.7874e-01, -2.7885e-01,\n",
              "         -6.6666e-01,  5.2950e-01,  9.9766e-01, -9.5589e-01, -2.1934e-01,\n",
              "         -9.9937e-01, -1.4924e-01, -8.3608e-01,  2.0334e-01, -5.0449e-01,\n",
              "          4.2333e-03, -7.8454e-01,  9.5188e-01,  2.2453e-01,  7.2209e-01,\n",
              "         -3.5845e-01,  9.4695e-01, -2.6168e-01, -6.1080e-02, -3.5684e-01,\n",
              "          1.8407e-02,  5.2775e-01,  1.9505e-01,  9.7894e-01, -9.4829e-01,\n",
              "          9.9977e-01, -3.2541e-01, -9.9993e-01, -9.9652e-01, -6.7036e-01,\n",
              "         -9.9954e-01,  6.8880e-01, -9.9493e-01,  9.8387e-01,  9.2372e-01,\n",
              "         -9.9744e-01, -9.9854e-01, -9.9707e-01, -9.9602e-01,  8.9889e-01,\n",
              "          4.8394e-01, -4.4900e-03, -2.9231e-01,  7.2057e-01, -5.0654e-03,\n",
              "         -1.2606e-01, -5.7995e-02, -9.2841e-01, -5.9745e-01, -9.9744e-01,\n",
              "          4.3539e-01, -9.9993e-01, -6.2040e-01,  9.9557e-01, -9.9301e-01,\n",
              "         -8.5156e-01, -9.1004e-01, -8.3913e-01, -8.1569e-01,  4.0908e-01,\n",
              "          9.8469e-01,  3.5833e-02, -5.8965e-01, -9.9935e-01,  9.7941e-01,\n",
              "         -6.0984e-01,  1.4569e-01, -8.9046e-01, -9.5852e-01,  9.9958e-01,\n",
              "          8.4597e-01, -7.9190e-02, -9.4438e-02, -9.9858e-01,  9.8247e-01,\n",
              "         -9.0012e-01, -9.1647e-01, -9.7822e-01,  1.6684e-01, -8.9643e-01,\n",
              "         -9.9974e-01,  3.4596e-02,  9.9470e-01,  9.9350e-01,  9.7613e-01,\n",
              "          3.3717e-01, -4.1559e-01, -9.4429e-01,  3.2113e-02, -9.9991e-01,\n",
              "          7.2102e-01,  7.8575e-01, -9.6309e-01, -6.2505e-01,  9.8769e-01,\n",
              "          9.6193e-01, -9.3011e-01, -9.7608e-01,  8.5422e-01,  6.7861e-01,\n",
              "          9.5998e-01, -5.0054e-01, -4.0896e-01,  4.2464e-01, -7.2000e-02,\n",
              "         -9.8670e-01, -8.7102e-01,  9.9342e-01, -9.9881e-01,  9.5390e-01,\n",
              "          9.9457e-01,  9.9807e-01,  1.1052e-04, -1.4544e-02, -9.8283e-01,\n",
              "         -9.9130e-01, -6.5011e-01,  3.7540e-01, -9.9988e-01,  9.9985e-01,\n",
              "         -9.9995e-01,  5.9670e-01, -5.9502e-01,  7.9122e-01,  9.8880e-01,\n",
              "         -3.3618e-01, -9.9983e-01, -9.9967e-01,  2.0326e-01, -5.6470e-02,\n",
              "          9.8409e-01,  1.7613e-01,  2.0839e-01, -4.6156e-01, -5.0810e-01,\n",
              "          9.9579e-01, -8.3887e-01, -6.8210e-01, -9.9716e-01,  9.9947e-01,\n",
              "          6.1443e-01, -9.9694e-01,  9.9328e-01, -9.9925e-01,  8.2647e-01,\n",
              "          9.6898e-01,  8.4211e-01,  9.7314e-01, -9.9827e-01,  9.9995e-01,\n",
              "         -9.9973e-01,  9.9605e-01, -9.9996e-01, -9.9868e-01,  9.9969e-01,\n",
              "         -9.8546e-01, -6.9904e-01, -9.9949e-01, -9.9709e-01,  4.8803e-01,\n",
              "          1.5219e-01, -5.1291e-01,  9.8327e-01, -9.9966e-01, -9.9759e-01,\n",
              "          6.0478e-01, -8.7154e-01, -8.2903e-01,  9.9451e-01, -4.0902e-01,\n",
              "          9.8453e-01, -2.0989e-01,  9.5186e-01,  9.6045e-02,  9.9647e-01,\n",
              "          9.9569e-01, -7.5884e-01, -4.0628e-01, -9.9020e-01,  9.8290e-01,\n",
              "         -5.1574e-01,  3.7787e-01,  9.3990e-01,  1.2247e-01, -4.9086e-01,\n",
              "          5.7078e-01, -9.9532e-01,  3.9396e-01, -3.8076e-01,  8.1643e-01,\n",
              "          9.2739e-01,  8.2285e-01, -4.4218e-03, -5.5771e-01, -2.1856e-01,\n",
              "         -9.9052e-01,  5.3940e-01, -9.9927e-01,  9.6673e-01, -9.4817e-01,\n",
              "          5.5289e-02, -4.3644e-01,  4.2865e-01, -9.5401e-01,  9.9933e-01,\n",
              "          9.9790e-01, -9.9847e-01,  1.0004e-01,  9.8212e-01, -5.3537e-01,\n",
              "          9.4653e-01, -9.8982e-01, -4.5346e-02,  9.6211e-01, -7.6799e-01,\n",
              "          9.7770e-01,  3.1095e-01, -1.1309e-01,  9.6038e-01, -9.9250e-01,\n",
              "         -8.6932e-01, -6.8875e-01,  2.6266e-01, -1.7082e-01, -9.5703e-01,\n",
              "          9.8051e-02,  9.7399e-01, -1.1300e-01, -9.9947e-01,  8.8466e-01,\n",
              "         -9.9898e-01, -1.5975e-01,  9.7102e-01,  4.3354e-02,  9.9986e-01,\n",
              "         -7.3476e-01,  8.7450e-02,  1.1875e-01, -9.9964e-01, -9.9843e-01,\n",
              "          9.7511e-02, -1.0685e-01, -8.4011e-01,  9.9858e-01, -1.3072e-01,\n",
              "          7.7312e-01, -9.9987e-01,  3.3419e-01,  9.9499e-01,  2.1689e-01,\n",
              "          7.5444e-01, -6.3101e-01, -9.2321e-01, -9.4333e-01, -6.0065e-01,\n",
              "          5.1700e-02,  8.5108e-01, -9.8022e-01, -8.3250e-01, -8.8058e-01,\n",
              "          9.9992e-01, -9.9674e-01, -9.1799e-01, -9.7941e-01,  3.6703e-01,\n",
              "          8.0833e-01,  4.3781e-01,  1.2856e-01, -8.6155e-01,  9.1621e-01,\n",
              "         -8.9923e-01,  9.9538e-01, -9.9306e-01, -9.9385e-01,  9.9962e-01,\n",
              "          6.4543e-01, -9.9127e-01, -2.5186e-01, -3.7328e-01,  2.3930e-01,\n",
              "          9.1351e-02,  6.9553e-01, -9.1190e-01, -2.0586e-01, -9.9683e-01,\n",
              "          8.6206e-01, -8.1761e-01, -9.8053e-01, -5.0257e-01, -3.6746e-01,\n",
              "         -9.9696e-01,  9.9117e-01,  9.6833e-01,  9.9991e-01, -9.9970e-01,\n",
              "          7.5103e-01,  1.1626e-01,  9.9859e-01,  4.8319e-02, -6.0784e-01,\n",
              "          8.5288e-01,  9.9947e-01, -6.6081e-01,  8.2227e-01, -1.1424e-01,\n",
              "         -4.0187e-02,  3.6690e-01, -7.5334e-01,  9.9591e-01, -9.0260e-01,\n",
              "          2.9268e-01, -9.6295e-01, -9.9985e-01,  9.9989e-01, -1.8609e-02,\n",
              "          9.8926e-01,  3.9779e-01,  8.1272e-01, -6.8693e-01,  9.7304e-01,\n",
              "         -9.7440e-01, -8.3143e-01, -9.9995e-01,  1.5942e-01, -9.9825e-01,\n",
              "         -9.8388e-01,  2.0992e-02,  9.7331e-01, -9.9935e-01, -9.8501e-01,\n",
              "         -3.8769e-01, -9.9996e-01,  9.3880e-01, -9.8498e-01, -7.7253e-01,\n",
              "         -9.8165e-01,  9.9667e-01, -1.9103e-01, -5.2293e-01,  9.6340e-01,\n",
              "         -9.5780e-01,  9.2354e-01,  9.3238e-01,  4.9094e-01,  2.5942e-01,\n",
              "          1.6537e-01, -7.6986e-01, -9.9392e-01, -8.9012e-01, -9.5096e-01,\n",
              "          6.4332e-01, -9.8298e-01, -6.9517e-01,  9.9451e-01,  9.8262e-01,\n",
              "         -9.9903e-01, -9.9363e-01,  9.9170e-01, -4.1454e-01,  9.8537e-01,\n",
              "         -3.9684e-01, -9.9976e-01, -9.9984e-01,  1.5351e-01, -1.7684e-01,\n",
              "          9.9146e-01, -3.8671e-01,  9.9295e-01,  6.8034e-01, -7.3334e-02,\n",
              "          4.1570e-01, -4.9775e-01, -1.5154e-01, -5.5098e-01, -1.5725e-01,\n",
              "          9.9994e-01, -7.0894e-01,  9.8472e-01]], grad_fn=<TanhBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLwzy5p5BEiZ"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96188jKBKN9"
      },
      "source": [
        "\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwQFZ0BFBMPT",
        "outputId": "7a1214ec-402d-4940-e5f9-781c25334365"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160])\n",
            "torch.Size([16, 160])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmywubDaBPTW",
        "outputId": "92981ee3-b032-4735-a3c6-8877a88cdd2f"
      },
      "source": [
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2620, 0.1210, 0.2518, 0.1417, 0.1688, 0.0547],\n",
              "        [0.2614, 0.0853, 0.1866, 0.1602, 0.2035, 0.1030],\n",
              "        [0.1835, 0.0980, 0.3478, 0.1001, 0.1965, 0.0742],\n",
              "        [0.3294, 0.0987, 0.2351, 0.1304, 0.1499, 0.0564],\n",
              "        [0.3255, 0.1119, 0.1279, 0.0922, 0.2651, 0.0773],\n",
              "        [0.2016, 0.1026, 0.3047, 0.1098, 0.2088, 0.0725],\n",
              "        [0.2173, 0.0815, 0.2391, 0.0836, 0.2993, 0.0793],\n",
              "        [0.1710, 0.1461, 0.2562, 0.1562, 0.1981, 0.0724],\n",
              "        [0.2196, 0.1068, 0.3483, 0.0972, 0.1530, 0.0751],\n",
              "        [0.1280, 0.1173, 0.2745, 0.1358, 0.3047, 0.0397],\n",
              "        [0.2208, 0.0844, 0.2324, 0.1292, 0.2652, 0.0680],\n",
              "        [0.2553, 0.1436, 0.2302, 0.0979, 0.2350, 0.0379],\n",
              "        [0.3037, 0.0537, 0.1901, 0.1950, 0.1908, 0.0667],\n",
              "        [0.2438, 0.0826, 0.2028, 0.1335, 0.2225, 0.1147],\n",
              "        [0.2429, 0.1011, 0.2441, 0.1712, 0.1756, 0.0650],\n",
              "        [0.2050, 0.1047, 0.2601, 0.1173, 0.2363, 0.0766]], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4flKY47zBSW1"
      },
      "source": [
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdZGoZ4TBjmJ"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBSyPenJHxSS"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlgSIw7RIRRg"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLkj5uqgH0FK",
        "outputId": "73b09583-517c-4f62-ec9d-47425d6e9179"
      },
      "source": [
        "#FOR 10 EPOCHS\n",
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 1.615551193555196 accuracy 0.3757791629563669\n",
            "Val   loss 1.580569360500727 accuracy 6.661290322580645\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 1.527145539338772 accuracy 0.4603739982190561\n",
            "Val   loss 1.3282357194484808 accuracy 10.403225806451612\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 1.3493385421924102 accuracy 0.5868210151380231\n",
            "Val   loss 1.0452465751232245 accuracy 13.403225806451612\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 1.0513984262943268 accuracy 0.7284060552092609\n",
            "Val   loss 0.7283390313386917 accuracy 15.580645161290322\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.7327479366690685 accuracy 0.8495102404274265\n",
            "Val   loss 0.47021360876850593 accuracy 17.370967741935484\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.5370466581139809 accuracy 0.9136242208370436\n",
            "Val   loss 0.36112456802183235 accuracy 18.112903225806452\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.39286458836152005 accuracy 0.9741763134461264\n",
            "Val   loss 0.27598334013078457 accuracy 18.532258064516128\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.3334536598995328 accuracy 0.98753339269813\n",
            "Val   loss 0.22115030351023263 accuracy 18.951612903225804\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.2479420884106404 accuracy 1.031166518254675\n",
            "Val   loss 0.1826386676552013 accuracy 19.177419354838708\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.21295387651293707 accuracy 1.0400712377560106\n",
            "Val   loss 0.16525732592130318 accuracy 19.370967741935484\n",
            "\n",
            "CPU times: user 4min 44s, sys: 4min 15s, total: 8min 59s\n",
            "Wall time: 9min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NjTuXOdoSrw",
        "outputId": "be05019e-dfae-467b-838e-57660e557822"
      },
      "source": [
        "#FOR 30 EPOCHS\n",
        "\n",
        "%%time\n",
        "EPOCHS = 30\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "Train loss 0.6165222764039078 accuracy 0.8967052537845057\n",
            "Val   loss 0.3248787737475374 accuracy 17.967741935483872\n",
            "\n",
            "Epoch 2/30\n",
            "----------\n",
            "Train loss 0.4795690779693616 accuracy 0.9483526268922529\n",
            "Val   loss 0.2653263221817234 accuracy 18.629032258064516\n",
            "\n",
            "Epoch 3/30\n",
            "----------\n",
            "Train loss 0.3061314686977615 accuracy 1.0044523597506678\n",
            "Val   loss 0.21798272853275427 accuracy 18.838709677419356\n",
            "\n",
            "Epoch 4/30\n",
            "----------\n",
            "Train loss 0.21780394749620405 accuracy 1.043633125556545\n",
            "Val   loss 0.11465265791719922 accuracy 19.564516129032256\n",
            "\n",
            "Epoch 5/30\n",
            "----------\n",
            "Train loss 0.1256025654192154 accuracy 1.0712377560106856\n",
            "Val   loss 0.04618122122244527 accuracy 19.870967741935484\n",
            "\n",
            "Epoch 6/30\n",
            "----------\n",
            "Train loss 0.05146688558101559 accuracy 1.0979519145146928\n",
            "Val   loss 0.02390758056814472 accuracy 19.967741935483872\n",
            "\n",
            "Epoch 7/30\n",
            "----------\n",
            "Train loss 0.03124280650286565 accuracy 1.1050756901157612\n",
            "Val   loss 0.014838062924517987 accuracy 20.048387096774192\n",
            "\n",
            "Epoch 8/30\n",
            "----------\n",
            "Train loss 0.02185748081254319 accuracy 1.1050756901157612\n",
            "Val   loss 0.011091338657141209 accuracy 20.064516129032256\n",
            "\n",
            "Epoch 9/30\n",
            "----------\n",
            "Train loss 0.01306326373499663 accuracy 1.1068566340160284\n",
            "Val   loss 0.008005644771634625 accuracy 20.08064516129032\n",
            "\n",
            "Epoch 10/30\n",
            "----------\n",
            "Train loss 0.00785103502522151 accuracy 1.1086375779162956\n",
            "Val   loss 0.004491194038284727 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 11/30\n",
            "----------\n",
            "Train loss 0.006017921865956548 accuracy 1.1095280498664293\n",
            "Val   loss 0.003699150780075564 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 12/30\n",
            "----------\n",
            "Train loss 0.007803478367023695 accuracy 1.1095280498664293\n",
            "Val   loss 0.004815169995708857 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 13/30\n",
            "----------\n",
            "Train loss 0.007924338036107842 accuracy 1.1095280498664293\n",
            "Val   loss 0.005186994737670876 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 14/30\n",
            "----------\n",
            "Train loss 0.00715594507197145 accuracy 1.1095280498664293\n",
            "Val   loss 0.0035888642507929127 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 15/30\n",
            "----------\n",
            "Train loss 0.003990969129984315 accuracy 1.1095280498664293\n",
            "Val   loss 0.003489743519681864 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 16/30\n",
            "----------\n",
            "Train loss 0.0038854869062794396 accuracy 1.1095280498664293\n",
            "Val   loss 0.004291828907205341 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 17/30\n",
            "----------\n",
            "Train loss 0.003865404253025563 accuracy 1.1095280498664293\n",
            "Val   loss 0.0043826618056277186 accuracy 20.08064516129032\n",
            "\n",
            "Epoch 18/30\n",
            "----------\n",
            "Train loss 0.005759168307681998 accuracy 1.1095280498664293\n",
            "Val   loss 0.005097289574992969 accuracy 20.08064516129032\n",
            "\n",
            "Epoch 19/30\n",
            "----------\n",
            "Train loss 0.0070634390315554366 accuracy 1.107747105966162\n",
            "Val   loss 0.004532951497448943 accuracy 20.08064516129032\n",
            "\n",
            "Epoch 20/30\n",
            "----------\n",
            "Train loss 0.005192819914485041 accuracy 1.1086375779162956\n",
            "Val   loss 0.00395649726338795 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 21/30\n",
            "----------\n",
            "Train loss 0.003524102236462148 accuracy 1.1095280498664293\n",
            "Val   loss 0.004308455029203264 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 22/30\n",
            "----------\n",
            "Train loss 0.005093118530613794 accuracy 1.1095280498664293\n",
            "Val   loss 0.0035505642567258268 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 23/30\n",
            "----------\n",
            "Train loss 0.00624799895815205 accuracy 1.1095280498664293\n",
            "Val   loss 0.003209836668154308 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 24/30\n",
            "----------\n",
            "Train loss 0.00433804038533335 accuracy 1.1086375779162956\n",
            "Val   loss 0.003283200188142916 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 25/30\n",
            "----------\n",
            "Train loss 0.003020869791492796 accuracy 1.1095280498664293\n",
            "Val   loss 0.003121030309744245 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 26/30\n",
            "----------\n",
            "Train loss 0.0034866633523518266 accuracy 1.1095280498664293\n",
            "Val   loss 0.0031898886829880304 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 27/30\n",
            "----------\n",
            "Train loss 0.0033995353306892915 accuracy 1.1095280498664293\n",
            "Val   loss 0.0032985337830047146 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 28/30\n",
            "----------\n",
            "Train loss 0.0038882382581598507 accuracy 1.1095280498664293\n",
            "Val   loss 0.0033466899503531377 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 29/30\n",
            "----------\n",
            "Train loss 0.003779639536496222 accuracy 1.1095280498664293\n",
            "Val   loss 0.0033590649065258615 accuracy 20.096774193548388\n",
            "\n",
            "Epoch 30/30\n",
            "----------\n",
            "Train loss 0.003450333099000347 accuracy 1.1095280498664293\n",
            "Val   loss 0.0033652945899237427 accuracy 20.096774193548388\n",
            "\n",
            "CPU times: user 14min 3s, sys: 12min 33s, total: 26min 37s\n",
            "Wall time: 27min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcp4ifYH4Jl"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVtuQsv6JamN"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch1jZRx_JnD5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}