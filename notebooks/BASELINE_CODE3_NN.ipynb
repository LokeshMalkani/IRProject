{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR MSR Rahul",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJGhlh8GB5Jk",
        "outputId": "12de4b11-92b5-460c-9ac4-da2e50427e04"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "!pip install demoji\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "\n",
        "nltk.download('all')\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: demoji in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from demoji) (0.4.4)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.18 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYcpGOJTCWDe"
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0f17sJTCWFx",
        "outputId": "94a72d79-b6db-4fe5-99d4-6de75b5cf88a"
      },
      "source": [
        "col_list = ['Text', 'Label']\n",
        "csvFile = '/content/drive/MyDrive/IIITD/IR/Project/Mid Sem/Data/Working_copy_22_MAR_Ver2.csv'\n",
        "csvFile = '/content/drive/MyDrive/IIITD/IR/Project/Mid Sem/Data/TRAIN_DATASET.csv'\n",
        "data =  pd.read_csv(csvFile, usecols=col_list)\n",
        "data.Text"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       This actually broke my heart... An elderly Kis...\n",
              "1       I am a daughter of farmers, of course Iâ€™m go...\n",
              "2       â€œthe power of people is stronger than the pe...\n",
              "3       Coming together is the beginning. Keeping toge...\n",
              "4       The farmers are more articulate and aware than...\n",
              "                              ...                        \n",
              "1243    AAP MLA @JarnailSinghAAP arrested for protesti...\n",
              "1244    This is a brutal and merciless attack by Modi ...\n",
              "1245    From the barbed wires\\n From the showers of wa...\n",
              "1246    Orders from above - use 'misguised' for #Farme...\n",
              "1247    AIKS is at the border with the Farmers. The Al...\n",
              "Name: Text, Length: 1248, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOU2yqsWZGoc"
      },
      "source": [
        "data = data[data.Label!='PROVOKING']"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJqswj2qZZcW",
        "outputId": "e4a4eecf-da27-44c5-ee2c-188e7661b873"
      },
      "source": [
        "data.Label.unique()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['PF_AG', 'NEUTRAL', 'PG_AF'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNeFtaCCWIe"
      },
      "source": [
        "#======================================================================================================\n",
        "                       \n",
        "                                       #PreProcessing\n",
        "\n",
        "#==============================================================================================\n",
        "def replaceElongated(word):\n",
        "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon(In Most Cases Working) \"\"\"\n",
        "\n",
        "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    repl = r'\\1\\2\\3'\n",
        "    if wordnet.synsets(word):\n",
        "        return word\n",
        "    repl_word = repeat_regexp.sub(repl, word)\n",
        "    if repl_word != word:      \n",
        "        return replaceElongated(repl_word)\n",
        "    else:       \n",
        "        return repl_word\n",
        "\n",
        "def doPreProcess(text):\n",
        "\n",
        "   # ----------------------[1.    Demoji]----------------------------------------------\n",
        "  text=demoji.replace_with_desc(text, sep=\"\")# Repalce with text\n",
        "\n",
        "\n",
        "  #------------------------[2.   # and @ handling here]--------------------------------\n",
        "  #Eiether-------------------------------------------\n",
        "    # text=re.sub(\"([@#][A-Za-z0-9]+)\",\"\",text)#Replace by space those words with # and @\n",
        "\n",
        "      #Or\n",
        "  text=re.sub(\"([@#])\",\"\",text)#Replace by space the chars as  # and @\n",
        "  #-----------------------------------------------------------\n",
        "\n",
        "\n",
        "  #------------------------[3.    Remove the Links]--------------------------------\n",
        "  text=re.sub(r\"http\\S+\", \"\", text)#Remove the Links\n",
        " \n",
        "\n",
        "  #-------------------------[4.   Emoticons Replacement with text]-------------------\n",
        "  \n",
        "\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  words=WhitespaceTokenizer().tokenize(text.lower())#tokenizer.tokenize(text)#Tokenize and casings\n",
        "  \n",
        "  #df = pd.read_excel('Emoticons_Emojis_Text.xlsx',sheet_name='Sheet2',header=0,converters={'Emoji':str,'Text':str,'Emoticons':str})\n",
        "  #dictOfEmoticons=df.set_index('Emoji')['Text'].to_dict()\n",
        "  #words=[dictOfEmoticons.get(x) if x in dictOfEmoticons.keys() else x for x in words]\n",
        "  text=' '.join(words)\n",
        "\n",
        "\n",
        "\n",
        "  #----------------------[5.       Ignore Non Ascii Characters]-----------------------\n",
        "  encoded_string = text.encode(\"ascii\", \"ignore\")  #For ASCII Onlys\n",
        "  text = encoded_string.decode()\n",
        "\n",
        "  #----------------------[6.       Single Quotes Handling Here Removal]---------------------:\n",
        "  text=text.translate(str.maketrans(\"\",\"\", \"'\"))\n",
        "  # text=text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "\n",
        "\n",
        "  #---------------------[7.         Tokenizes]----------------------\n",
        "  # tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  words=WhitespaceTokenizer().tokenize(text)\n",
        " \n",
        "  #---------------------[8.         Lower Case Folding]------------------------------\n",
        "  # words=tokenizer.tokenize(text.lower())#Tokenize and casings\n",
        "\n",
        "  #---------------------[9.         Replace Elongated Words]-------------------------------\n",
        "\n",
        "  words=[replaceElongated(x) if not wordnet.synsets(x) else x for x in  words]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   #---------------------[10. & 11.        Stopwords Removal & Lematize]--------------------------------------------- \n",
        "  wnl = WordNetLemmatizer() \n",
        "  words=[wnl.lemmatize(word) for word in words]\n",
        "  words=[word for word in words if word not in stopwords.words('english') ]\n",
        "\n",
        "\n",
        "\n",
        "  #---------------------[10. & 11.        Stopwords Removal & Lematize]--------------------------------------------- \n",
        "  # wnl = WordNetLemmatizer() \n",
        "  # words=[wnl.lemmatize(word) for word in words]\n",
        "  # words=[word for word in words if word not in stopwords.words('english') ]\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  #finalText= ' '.join(all_words)\n",
        "\n",
        "  #-------------------[11.                 Pnctations Removal]------------------------------\n",
        "  #return re.sub(r'[^\\w\\s]', '',words)\n",
        "  \n",
        "  #return words\n",
        "  return ' '.join(words)\n",
        "\n",
        "def preProcessing(df):\n",
        "  \n",
        "  df=df[['Text']]['Text'].apply(doPreProcess)\n",
        "  return df\n",
        "\n",
        "#======================================================================================================\n",
        "                       \n",
        "                                       #Global Data Frames\n",
        "\n",
        "#=============================================================================================="
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tThEg0pJCWK6"
      },
      "source": [
        "data['Preprocessed_tweets']=data.Text.apply(doPreProcess)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fU9yieRCWNd"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "df = data\n",
        "df.columns = ['Text', 'Label', 'Tweets']"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1q5n2BICWPk"
      },
      "source": [
        "indexDictionary = {}\n",
        "index = 0\n",
        "for cell in df.Tweets:\n",
        "  for word in cell.split(' '):\n",
        "    if not (word in indexDictionary):\n",
        "      indexDictionary[word] = index\n",
        "      index+=1"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDn-06BmCWR6"
      },
      "source": [
        "#String to list seperated by space\n",
        "finalList = []\n",
        "for cell in data.Tweets:\n",
        "  cellList = []\n",
        "  for word in cell.split(' '):\n",
        "    cellList.append(indexDictionary[word])\n",
        "  finalList.append(cellList)\n",
        "  "
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSmuSVxMCWUK",
        "outputId": "3f8ca162-b608-4eac-e029-c77793e7db02"
      },
      "source": [
        "#Numpy array of word indexes\n",
        "ndata = np.array(finalList)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9yeLjoQCWXf"
      },
      "source": [
        "#Replace ANTI_GOVT, ANTI_FARMERS... with 0, 1, 2...\n",
        "uniqueNames = data.Label.unique()\n",
        "for i in range(len(uniqueNames)):\n",
        "  df.Label = df.Label.replace(uniqueNames[i], i)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnYo8wpvCWaz"
      },
      "source": [
        "#Numpy array of targets\n",
        "targets = df.Label.to_numpy()"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VItJNQb5CWdU"
      },
      "source": [
        "dimensions = 8000\n",
        "def vectorizeData(sequences, dimension = dimensions):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1\n",
        "    return results\n",
        "    \n",
        "#Vectorize data\n",
        "data = vectorizeData(ndata)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO-tELQ8E2IF"
      },
      "source": [
        "#Divide into train test with random\n",
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(data, targets, test_size=0.33, random_state = 4)\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH6Ekt5LE2K5",
        "outputId": "0d2d41e9-4899-4a00-b292-4ddde873d6be"
      },
      "source": [
        "#Divide into train test with random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(data, targets, test_size=0.33, random_state = 4)\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(8000, )))\n",
        "\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "model.compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "results = model.fit(\n",
        "    xTrain, yTrain,\n",
        "    epochs= 2,\n",
        "    batch_size = 500,\n",
        "    validation_data = (xTest, yTest)\n",
        ")\n",
        "\n",
        "\n",
        "print(np.mean(results.history[\"accuracy\"]))\n",
        "model.summary()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.5314WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8095c9ed40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.0000e+00 - accuracy: 0.5326 - val_loss: 0.0000e+00 - val_accuracy: 0.5909\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0000e+00 - accuracy: 0.5448 - val_loss: 0.0000e+00 - val_accuracy: 0.5909\n",
            "0.5410447716712952\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 50)                400050    \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 405,201\n",
            "Trainable params: 405,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dotq6zkPE2UY",
        "outputId": "9d62be8f-75ff-4160-d2d6-c3beac681a09"
      },
      "source": [
        "import pickle\n",
        "def saveData(pic, filename):\n",
        "  with open(filename, 'wb') as file:\n",
        "      pickle.dump(pic, file)\n",
        "\n",
        "pathName = '/content/drive/MyDrive/IIITD/IR/Project/Mid Sem/Data/'\n",
        "\n",
        "saveData(data, pathName+'numpyData.sav')\n",
        "saveData(targets, pathName+'numpyTargets.sav')\n",
        "\n",
        "model.save(pathName + 'model.sav')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/IIITD/IR/Project/Mid Sem/Data/model.sav/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRg57kzdE2Xk"
      },
      "source": [
        "def loadData(filename):\n",
        "  with open(filename, 'rb') as file:\n",
        "      return pickle.load(file)\n",
        "\n",
        "data = loadData(pathName+'numpyData.sav')\n",
        "targets = loadData(pathName+'numpyTargets.sav')\n",
        "model = keras.models.load_model(pathName + 'model.sav')"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41741iJTkdTZ",
        "outputId": "a6a7e857-f65f-49f2-acb8-0cb18e9093db"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(data, targets, test_size=0.33, random_state = 4)\n",
        "\n",
        "results = model.fit(\n",
        "    xTrain, yTrain,\n",
        "    epochs= 2,\n",
        "    batch_size = 500,\n",
        "    validation_data = (xTest, yTest)\n",
        ")\n",
        "\n",
        "\n",
        "print(np.mean(results.history[\"accuracy\"]))\n",
        "model.summary()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.5860WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f80834b08c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.0000e+00 - accuracy: 0.5473 - val_loss: 0.0000e+00 - val_accuracy: 0.5909\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 0.0000e+00 - accuracy: 0.5473 - val_loss: 0.0000e+00 - val_accuracy: 0.5909\n",
            "0.5472636818885803\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 50)                400050    \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 405,201\n",
            "Trainable params: 405,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9508BXlqPaJ"
      },
      "source": [
        "savedModelLink = 'https://drive.google.com/drive/folders/1fH2x6vKV1tXLPCdAjLpGC5pi8UesE__9?usp=sharing'"
      ],
      "execution_count": 150,
      "outputs": []
    }
  ]
}